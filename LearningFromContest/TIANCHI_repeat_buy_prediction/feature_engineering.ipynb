{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "import gc\n",
    "from collections import Counter\n",
    "import copy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"./data/test_format1.csv\")\n",
    "train_data = pd.read_csv(\"./data/train_format1.csv\")\n",
    "user_info = pd.read_csv(\"./data/user_info_format1.csv\")\n",
    "user_log = pd.read_csv(\"./data/user_log_format1.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "# 对数据内存压缩\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print(\"Memory usage after optimation is {:.2f} MB\".format(end_mem))\n",
    "    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimation is 1.74 MB\n",
      "Decreased by 70.8%\n",
      "Memory usage after optimation is 3.49 MB\n",
      "Decreased by 41.7%\n",
      "Memory usage after optimation is 3.24 MB\n",
      "Decreased by 66.7%\n",
      "Memory usage after optimation is 890.48 MB\n",
      "Decreased by 69.6%\n"
     ]
    }
   ],
   "source": [
    "train_data = reduce_mem_usage(train_data)\n",
    "test_data = reduce_mem_usage(test_data)\n",
    "user_info = reduce_mem_usage(user_info)\n",
    "user_log = reduce_mem_usage(user_log)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "<module 'gc' (built-in)>"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del test_data['prob']\n",
    "all_data = train_data.append(test_data)\n",
    "all_data = all_data.merge(user_info, on=['user_id'], how='left')\n",
    "del train_data, test_data, user_info\n",
    "gc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "user_log = user_log.sort_values(['user_id', 'time_stamp'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "list_join_func = lambda x: \" \".join([str(i) for i in x])\n",
    "\n",
    "agg_dict = {\n",
    "    'item_id': list_join_func,\n",
    "    'cat_id': list_join_func,\n",
    "    'seller_id': list_join_func,\n",
    "    'brand_id': list_join_func,\n",
    "    'time_stamp': list_join_func,\n",
    "    'action_type': list_join_func\n",
    "}\n",
    "\n",
    "rename_dict = {\n",
    "    'item_id': 'item_path',\n",
    "    'cat_id': 'cat_path',\n",
    "    'seller_id': 'seller_path',\n",
    "    'brand_id': 'brand_path',\n",
    "    'time_stamp': 'time_stamp_path',\n",
    "    'action_type': 'action_type_path'\n",
    "}\n",
    "\n",
    "user_log_path = user_log.groupby('user_id').agg(agg_dict).reset_index().rename(columns=rename_dict)\n",
    "# def merge_list(df_ID, join_columns, df_data, agg_dict, rename_dict):\n",
    "#     df_data = df_data.\\\n",
    "#         groupby(join_columns).\\\n",
    "#         agg(agg_dict).\\\n",
    "#         reset_index().\\\n",
    "#         rename(columns=rename_dict)\n",
    "#     df_ID = df_ID.merge(df_data, on=join_columns, how='left')\n",
    "#     return df_ID\n",
    "#\n",
    "# print(all_data.head())\n",
    "# all_data = merge_list(all_data, 'user_id', user_log, agg_dict, rename_dict)\n",
    "# print(all_data.head())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "data": {
      "text/plain": "221"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_path = all_data.merge(user_log_path,on='user_id')\n",
    "del user_log\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "def cnt_(x):\n",
    "    try:\n",
    "        return len(x.split(' '))\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def nunique_(x):\n",
    "    try:\n",
    "        return len(set(x.split(' ')))\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def max_(x):\n",
    "    try:\n",
    "        return np.max([float(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def min_(x):\n",
    "    try:\n",
    "        return np.min([float(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def std_(x):\n",
    "    try:\n",
    "        return np.std([float(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def most_n(x, n):\n",
    "    try:\n",
    "        return Counter(x.split(' ')).most_common(n)[n-1][0]\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def most_n_cnt(x, n):\n",
    "    try:\n",
    "        return Counter(x.split(' ')).most_common(n)[n-1][1]\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def user_cnt(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(cnt_)\n",
    "    return df_data\n",
    "\n",
    "def user_unique(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(nunique_)\n",
    "    return df_data\n",
    "\n",
    "def user_max(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(max_)\n",
    "    return df_data\n",
    "\n",
    "def user_min(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(min_)\n",
    "    return df_data\n",
    "\n",
    "def user_std(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(std_)\n",
    "    return df_data\n",
    "\n",
    "def user_most_n(df_data, single_col, name, n=1):\n",
    "    func = lambda x: most_n(x, n)\n",
    "    df_data[name] = df_data[single_col].apply(func)\n",
    "    return df_data\n",
    "\n",
    "def user_most_n_cnt(df_data, single_col, name, n=1):\n",
    "    func = lambda x: most_n_cnt(x, n)\n",
    "    df_data[name] = df_data[single_col].apply(func)\n",
    "    return df_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "all_data_test = all_data_path.head(2000)\n",
    "all_data_test = user_cnt(all_data_test, 'seller_path', 'user_cnt')\n",
    "all_data_test = user_unique(all_data_test, 'seller_path', 'seller_nunique')\n",
    "all_data_test = user_unique(all_data_test, 'cat_path', 'cat_nunique')\n",
    "all_data_test = user_unique(all_data_test, 'brand_path', 'brand_nunique')\n",
    "all_data_test = user_unique(all_data_test, 'item_path', 'item_nunique')\n",
    "all_data_test = user_unique(all_data_test, 'time_stamp_path', 'time_stamp_nunique')\n",
    "all_data_test = user_unique(all_data_test, 'action_type_path', 'action_type_nunique')\n",
    "\n",
    "all_data_test = user_max(all_data_test, 'action_type_path', 'time_stamp_max')\n",
    "all_data_test = user_min(all_data_test, 'action_type_path', 'time_stamp_min')\n",
    "all_data_test = user_std(all_data_test, 'action_type_path', 'time_stamp_std')\n",
    "all_data_test['time_stamp_range'] = all_data_test['time_stamp_max'] - all_data_test['time_stamp_min']\n",
    "all_data_test = user_most_n(all_data_test, 'seller_path', 'seller_most_1', 1)\n",
    "all_data_test = user_most_n(all_data_test, 'cat_path', 'cat_most_1', 1)\n",
    "all_data_test = user_most_n(all_data_test, 'brand_path', 'brand_most_1', 1)\n",
    "all_data_test = user_most_n(all_data_test, 'action_type_path', 'action_type_1', 1)\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'seller_path', 'seller_most_1_cnt', 1)\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'cat_path', 'cat_most_1_cnt', 1)\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'brand_path', 'brand_most_1_cnt', 1)\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'action_type_path', 'action_type_1_cnt', 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "def col_cnt_(df_data, columns_list, action_type):\n",
    "    try:\n",
    "        data_dict = {}\n",
    "        col_list = copy.deepcopy(columns_list)\n",
    "        if action_type != None:\n",
    "            col_list += ['action_type_path']\n",
    "\n",
    "        for col in col_list:\n",
    "            data_dict[col] = df_data[col].split(' ')\n",
    "\n",
    "        path_len = len(data_dict[col])\n",
    "\n",
    "        data_out = []\n",
    "        for i_ in range(path_len):\n",
    "            data_txt = ''\n",
    "            for col_ in columns_list:\n",
    "                if data_dict['action_type_path'][i_] == action_type:\n",
    "                    data_txt += '_' + data_dict[col_][i_]\n",
    "            data_out.append(data_txt)\n",
    "        return len(data_out)\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def col_nuique_(df_data, columns_list, action_type):\n",
    "    try:\n",
    "        data_dict = {}\n",
    "\n",
    "        col_list = copy.deepcopy(columns_list)\n",
    "        if action_type != None:\n",
    "            col_list += ['action_type_path']\n",
    "\n",
    "        for col in col_list:\n",
    "            data_dict[col] = df_data[col].split(' ')\n",
    "\n",
    "        path_len = len(data_dict[col])\n",
    "\n",
    "        data_out = []\n",
    "        for i_ in range(path_len):\n",
    "            data_txt = ''\n",
    "            for col_ in columns_list:\n",
    "                if data_dict['action_type_path'][i_] == action_type:\n",
    "                    data_txt += '_' + data_dict[col_][i_]\n",
    "            data_out.append(data_txt)\n",
    "\n",
    "        return len(set(data_out))\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def user_col_cnt(df_data, columns_list, action_type, name):\n",
    "    df_data[name] = df_data.apply(lambda x: col_cnt_(x, columns_list, action_type), axis=1)\n",
    "    return df_data\n",
    "\n",
    "def user_col_nunique(df_data, columns_list, action_type, name):\n",
    "    df_data[name] = df_data.apply(lambda x: col_nuique_(x, columns_list, action_type), axis=1)\n",
    "    return df_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "# 点击次数\n",
    "all_data_test = user_col_cnt(all_data_test,  ['seller_path'], '0', 'user_cnt_0')\n",
    "# 加购次数\n",
    "all_data_test = user_col_cnt(all_data_test,  ['seller_path'], '1', 'user_cnt_1')\n",
    "# 购买次数\n",
    "all_data_test = user_col_cnt(all_data_test,  ['seller_path'], '2', 'user_cnt_2')\n",
    "# 收藏次数\n",
    "all_data_test = user_col_cnt(all_data_test,  ['seller_path'], '3', 'user_cnt_3')\n",
    "\n",
    "# 不同店铺个数\n",
    "all_data_test = user_col_nunique(all_data_test,  ['seller_path'], '0', 'seller_nunique_0')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['user_id', 'merchant_id', 'label', 'age_range', 'gender', 'item_path',\n       'cat_path', 'seller_path', 'brand_path', 'time_stamp_path',\n       'action_type_path', 'user_cnt', 'seller_nunique', 'cat_nunique',\n       'brand_nunique', 'item_nunique', 'time_stamp_nunique',\n       'action_type_nunique', 'time_stamp_max', 'time_stamp_min',\n       'time_stamp_std', 'time_stamp_range', 'seller_most_1', 'cat_most_1',\n       'brand_most_1', 'action_type_1', 'seller_most_1_cnt', 'cat_most_1_cnt',\n       'brand_most_1_cnt', 'action_type_1_cnt', 'user_cnt_0', 'user_cnt_1',\n       'user_cnt_2', 'user_cnt_3', 'seller_nunique_0'],\n      dtype='object')"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_test.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "# 利用Countvector和TF-IDF提取特征\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from scipy import sparse\n",
    "\n",
    "tfidfVec = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS,\n",
    "                           ngram_range=(1, 1),\n",
    "                           max_features=100)\n",
    "\n",
    "columns_list = ['seller_path']\n",
    "for i, col in enumerate(columns_list):\n",
    "    tfidfVec.fit(all_data_test[col])\n",
    "    data_ = tfidfVec.transform(all_data_test[col])\n",
    "    if i == 0:\n",
    "        data_cat = data_\n",
    "    else:\n",
    "        data_cat = sparse.hstack((data_cat, data_))\n",
    "\n",
    "df_tfidf = pd.DataFrame(data_cat.toarray())\n",
    "df_tfidf.columns = ['tfidf_' + str(i) for i in df_tfidf.columns]\n",
    "all_data_test = pd.concat([all_data_test, df_tfidf], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "model = gensim.models.Word2Vec(\n",
    "    all_data_test[\"seller_path\"].apply(lambda x: x.split(' ')),\n",
    "    # size = 100,\n",
    "    window = 5,\n",
    "    min_count = 5,\n",
    "    workers = 4\n",
    ")\n",
    "\n",
    "def mean_w2v_(x, model, size=100):\n",
    "    try:\n",
    "        i = 0\n",
    "        for word in x.split(' '):\n",
    "            if word in model.wv.vocab:\n",
    "                i += 1\n",
    "                if i == 1:\n",
    "                    vec = np.zeros(size)\n",
    "                vec += model.wv[word]\n",
    "        return vec / i\n",
    "    except:\n",
    "        return np.zeros(size)\n",
    "\n",
    "def get_mean_w2v(df_data, columns, model, size):\n",
    "    data_array = []\n",
    "    for index, row in df_data.iterrows():\n",
    "        w2v = mean_w2v_(row[columns], model, size)\n",
    "        data_array.append(w2v)\n",
    "    return pd.DataFrame(data_array)\n",
    "\n",
    "df_embedding = get_mean_w2v(all_data_test, 'seller_path', model, 100)\n",
    "df_embedding.columns = ['embedding_' + str(i) for i in df_embedding.columns]\n",
    "\n",
    "all_data_test = pd.concat([all_data_test, df_embedding], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "# Stacking 分类特征\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import xgboost\n",
    "import lightgbm\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier,ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor,GradientBoostingRegressor,ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import log_loss,mean_absolute_error,mean_squared_error\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "folds = 5\n",
    "seed = 1\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "\"\"\"\n",
    "-- 分类\n",
    "-- stacking 分类特征\n",
    "\"\"\"\n",
    "def stacking_clf(clf,train_x,train_y,test_x,clf_name,kf,label_split=None):\n",
    "    train=np.zeros((train_x.shape[0],1))\n",
    "    test=np.zeros((test_x.shape[0],1))\n",
    "    test_pre=np.empty((folds, test_x.shape[0],1))\n",
    "    cv_scores=[]\n",
    "    for i,(train_index,test_index) in enumerate(kf.split(train_x,label_split)):\n",
    "        tr_x=train_x[train_index]\n",
    "        tr_y=train_y[train_index]\n",
    "        te_x=train_x[test_index]\n",
    "        te_y = train_y[test_index]\n",
    "\n",
    "        if clf_name in [\"rf\",\"ada\",\"gb\",\"et\",\"lr\",\"knn\",\"gnb\"]:\n",
    "            clf.fit(tr_x,tr_y)\n",
    "            pre=clf.predict_proba(te_x)\n",
    "\n",
    "            train[test_index]=pre[:,0].reshape(-1,1)\n",
    "            test_pre[i,:]=clf.predict_proba(test_x)[:,0].reshape(-1,1)\n",
    "\n",
    "            cv_scores.append(log_loss(te_y, pre[:,0].reshape(-1,1)))\n",
    "        elif clf_name in [\"xgb\"]:\n",
    "            train_matrix = clf.DMatrix(tr_x, label=tr_y, missing=-1)\n",
    "            test_matrix = clf.DMatrix(te_x, label=te_y, missing=-1)\n",
    "            z = clf.DMatrix(test_x)\n",
    "            params = {'booster': 'gbtree',\n",
    "                      'objective': 'multi:softprob',\n",
    "                      'eval_metric': 'mlogloss',\n",
    "                      'gamma': 1,\n",
    "                      'min_child_weight': 1.5,\n",
    "                      'max_depth': 5,\n",
    "                      'lambda': 10,\n",
    "                      'subsample': 0.7,\n",
    "                      'colsample_bytree': 0.7,\n",
    "                      'colsample_bylevel': 0.7,\n",
    "                      'eta': 0.03,\n",
    "                      'tree_method': 'exact',\n",
    "                      'seed': 2017,\n",
    "                      \"num_class\": 2\n",
    "                      }\n",
    "\n",
    "            num_round = 10000\n",
    "            early_stopping_rounds = 100\n",
    "            watchlist = [(train_matrix, 'train'),\n",
    "                         (test_matrix, 'eval')\n",
    "                         ]\n",
    "            if test_matrix:\n",
    "                model = clf.train(params, train_matrix, num_boost_round=num_round,evals=watchlist,\n",
    "                                  early_stopping_rounds=early_stopping_rounds\n",
    "                                  )\n",
    "                pre= model.predict(test_matrix,ntree_limit=model.best_ntree_limit)\n",
    "                train[test_index]=pre[:,0].reshape(-1,1)\n",
    "                test_pre[i, :]= model.predict(z, ntree_limit=model.best_ntree_limit)[:,0].reshape(-1,1)\n",
    "                cv_scores.append(log_loss(te_y, pre[:,0].reshape(-1,1)))\n",
    "        elif clf_name in [\"lgb\"]:\n",
    "            train_matrix = clf.Dataset(tr_x, label=tr_y)\n",
    "            test_matrix = clf.Dataset(te_x, label=te_y)\n",
    "            params = {\n",
    "                      'boosting_type': 'gbdt',\n",
    "                      #'boosting_type': 'dart',\n",
    "                      'objective': 'multiclass',\n",
    "                      'metric': 'multi_logloss',\n",
    "                      'min_child_weight': 1.5,\n",
    "                      'num_leaves': 2**5,\n",
    "                      'lambda_l2': 10,\n",
    "                      'subsample': 0.7,\n",
    "                      'colsample_bytree': 0.7,\n",
    "                      'colsample_bylevel': 0.7,\n",
    "                      'learning_rate': 0.03,\n",
    "                      'tree_method': 'exact',\n",
    "                      'seed': 2017,\n",
    "                      \"num_class\": 2,\n",
    "                      'silent': True,\n",
    "                      }\n",
    "            num_round = 10000\n",
    "            early_stopping_rounds = 100\n",
    "            if test_matrix:\n",
    "                model = clf.train(params, train_matrix,num_round,valid_sets=test_matrix,\n",
    "                                  early_stopping_rounds=early_stopping_rounds\n",
    "                                  )\n",
    "                pre= model.predict(te_x,num_iteration=model.best_iteration)\n",
    "                train[test_index]=pre[:,0].reshape(-1,1)\n",
    "                test_pre[i, :]= model.predict(test_x, num_iteration=model.best_iteration)[:,0].reshape(-1,1)\n",
    "                cv_scores.append(log_loss(te_y, pre[:,0].reshape(-1,1)))\n",
    "        else:\n",
    "            raise IOError(\"Please add new clf.\")\n",
    "        print(\"%s now score is:\"%clf_name,cv_scores)\n",
    "    test[:]=test_pre.mean(axis=0)\n",
    "    print(\"%s_score_list:\"%clf_name,cv_scores)\n",
    "    print(\"%s_score_mean:\"%clf_name,np.mean(cv_scores))\n",
    "    return train.reshape(-1,1),test.reshape(-1,1)\n",
    "\n",
    "def rf_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    randomforest = RandomForestClassifier(n_estimators=1200, max_depth=20, n_jobs=-1, random_state=2017, max_features=\"auto\",verbose=1)\n",
    "    rf_train, rf_test = stacking_clf(randomforest, x_train, y_train, x_valid, \"rf\", kf, label_split=label_split)\n",
    "    return rf_train, rf_test,\"rf\"\n",
    "\n",
    "def ada_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    adaboost = AdaBoostClassifier(n_estimators=50, random_state=2017, learning_rate=0.01)\n",
    "    ada_train, ada_test = stacking_clf(adaboost, x_train, y_train, x_valid, \"ada\", kf, label_split=label_split)\n",
    "    return ada_train, ada_test,\"ada\"\n",
    "\n",
    "def gb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    gbdt = GradientBoostingClassifier(learning_rate=0.04, n_estimators=100, subsample=0.8, random_state=2017,max_depth=5,verbose=1)\n",
    "    gbdt_train, gbdt_test = stacking_clf(gbdt, x_train, y_train, x_valid, \"gb\", kf, label_split=label_split)\n",
    "    return gbdt_train, gbdt_test,\"gb\"\n",
    "\n",
    "def et_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    extratree = ExtraTreesClassifier(n_estimators=1200, max_depth=35, max_features=\"auto\", n_jobs=-1, random_state=2017,verbose=1)\n",
    "    et_train, et_test = stacking_clf(extratree, x_train, y_train, x_valid, \"et\", kf, label_split=label_split)\n",
    "    return et_train, et_test,\"et\"\n",
    "\n",
    "def xgb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    xgb_train, xgb_test = stacking_clf(xgboost, x_train, y_train, x_valid, \"xgb\", kf, label_split=label_split)\n",
    "    return xgb_train, xgb_test,\"xgb\"\n",
    "\n",
    "def lgb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    xgb_train, xgb_test = stacking_clf(lightgbm, x_train, y_train, x_valid, \"lgb\", kf, label_split=label_split)\n",
    "    return xgb_train, xgb_test,\"lgb\"\n",
    "\n",
    "def gnb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    gnb=GaussianNB()\n",
    "    gnb_train, gnb_test = stacking_clf(gnb, x_train, y_train, x_valid, \"gnb\", kf, label_split=label_split)\n",
    "    return gnb_train, gnb_test,\"gnb\"\n",
    "\n",
    "def lr_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    logisticregression=LogisticRegression(n_jobs=-1,random_state=2017,C=0.1,max_iter=200)\n",
    "    lr_train, lr_test = stacking_clf(logisticregression, x_train, y_train, x_valid, \"lr\", kf, label_split=label_split)\n",
    "    return lr_train, lr_test, \"lr\"\n",
    "\n",
    "def knn_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    kneighbors=KNeighborsClassifier(n_neighbors=200,n_jobs=-1)\n",
    "    knn_train, knn_test = stacking_clf(kneighbors, x_train, y_train, x_valid, \"lr\", kf, label_split=label_split)\n",
    "    return knn_train, knn_test, \"knn\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "features_columns = [c for c in all_data_test.columns if c not in ['label', 'prob', 'seller_path', 'cat_path', 'brand_path', 'action_type_path', 'item_path', 'time_stamp_path']]\n",
    "x_train = all_data_test[~all_data_test['label'].isna()][features_columns].values\n",
    "y_train = all_data_test[~all_data_test['label'].isna()]['label'].values\n",
    "x_valid = all_data_test[all_data_test['label'].isna()][features_columns].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [],
   "source": [
    "def get_matrix(data):\n",
    "    where_are_nan = np.isnan(data)\n",
    "    where_are_inf = np.isinf(data)\n",
    "    data[where_are_nan] = 0\n",
    "    data[where_are_inf] = 0\n",
    "    return data\n",
    "x_train = np.float_(get_matrix(np.float_(x_train)))\n",
    "y_train = np.int_(y_train)\n",
    "x_valid = x_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [],
   "source": [
    "clf_list = [lgb_clf, xgb_clf]\n",
    "clf_list_col = ['lgb_clf', 'xgb_clf']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001089 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6491\n",
      "[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 125\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Start training from score -0.066541\n",
      "[LightGBM] [Info] Start training from score -2.743030\n",
      "[1]\tvalid_0's multi_logloss: 0.240441\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's multi_logloss: 0.240198\n",
      "[3]\tvalid_0's multi_logloss: 0.240026\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's multi_logloss: 0.239927\n",
      "[5]\tvalid_0's multi_logloss: 0.240004\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's multi_logloss: 0.239939\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's multi_logloss: 0.239708\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's multi_logloss: 0.239764\n",
      "[9]\tvalid_0's multi_logloss: 0.239771\n",
      "[10]\tvalid_0's multi_logloss: 0.239644\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's multi_logloss: 0.239537\n",
      "[12]\tvalid_0's multi_logloss: 0.239589\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's multi_logloss: 0.239487\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's multi_logloss: 0.239472\n",
      "[15]\tvalid_0's multi_logloss: 0.239183\n",
      "[16]\tvalid_0's multi_logloss: 0.239134\n",
      "[17]\tvalid_0's multi_logloss: 0.239042\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's multi_logloss: 0.239262\n",
      "[19]\tvalid_0's multi_logloss: 0.23913\n",
      "[20]\tvalid_0's multi_logloss: 0.239172\n",
      "[21]\tvalid_0's multi_logloss: 0.239094\n",
      "[22]\tvalid_0's multi_logloss: 0.238882\n",
      "[23]\tvalid_0's multi_logloss: 0.23903\n",
      "[24]\tvalid_0's multi_logloss: 0.239025\n",
      "[25]\tvalid_0's multi_logloss: 0.239409\n",
      "[26]\tvalid_0's multi_logloss: 0.239516\n",
      "[27]\tvalid_0's multi_logloss: 0.239661\n",
      "[28]\tvalid_0's multi_logloss: 0.239751\n",
      "[29]\tvalid_0's multi_logloss: 0.239552\n",
      "[30]\tvalid_0's multi_logloss: 0.239832\n",
      "[31]\tvalid_0's multi_logloss: 0.239983\n",
      "[32]\tvalid_0's multi_logloss: 0.240251\n",
      "[33]\tvalid_0's multi_logloss: 0.240564\n",
      "[34]\tvalid_0's multi_logloss: 0.240592\n",
      "[35]\tvalid_0's multi_logloss: 0.240844\n",
      "[36]\tvalid_0's multi_logloss: 0.240947\n",
      "[37]\tvalid_0's multi_logloss: 0.241056\n",
      "[38]\tvalid_0's multi_logloss: 0.24135\n",
      "[39]\tvalid_0's multi_logloss: 0.241555\n",
      "[40]\tvalid_0's multi_logloss: 0.241648\n",
      "[41]\tvalid_0's multi_logloss: 0.241315\n",
      "[42]\tvalid_0's multi_logloss: 0.241436\n",
      "[43]\tvalid_0's multi_logloss: 0.241415\n",
      "[44]\tvalid_0's multi_logloss: 0.241383\n",
      "[45]\tvalid_0's multi_logloss: 0.241514\n",
      "[46]\tvalid_0's multi_logloss: 0.241697\n",
      "[47]\tvalid_0's multi_logloss: 0.241614\n",
      "[48]\tvalid_0's multi_logloss: 0.241999\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[49]\tvalid_0's multi_logloss: 0.242248\n",
      "[50]\tvalid_0's multi_logloss: 0.242512\n",
      "[51]\tvalid_0's multi_logloss: 0.242635\n",
      "[52]\tvalid_0's multi_logloss: 0.242752\n",
      "[53]\tvalid_0's multi_logloss: 0.242775\n",
      "[54]\tvalid_0's multi_logloss: 0.242683\n",
      "[55]\tvalid_0's multi_logloss: 0.242925\n",
      "[56]\tvalid_0's multi_logloss: 0.243029\n",
      "[57]\tvalid_0's multi_logloss: 0.243378\n",
      "[58]\tvalid_0's multi_logloss: 0.243525\n",
      "[59]\tvalid_0's multi_logloss: 0.243467\n",
      "[60]\tvalid_0's multi_logloss: 0.243572\n",
      "[61]\tvalid_0's multi_logloss: 0.243761\n",
      "[62]\tvalid_0's multi_logloss: 0.243891\n",
      "[63]\tvalid_0's multi_logloss: 0.243864\n",
      "[64]\tvalid_0's multi_logloss: 0.244053\n",
      "[65]\tvalid_0's multi_logloss: 0.244328\n",
      "[66]\tvalid_0's multi_logloss: 0.24424\n",
      "[67]\tvalid_0's multi_logloss: 0.244191\n",
      "[68]\tvalid_0's multi_logloss: 0.244326\n",
      "[69]\tvalid_0's multi_logloss: 0.244216\n",
      "[70]\tvalid_0's multi_logloss: 0.24426\n",
      "[71]\tvalid_0's multi_logloss: 0.244434\n",
      "[72]\tvalid_0's multi_logloss: 0.244564\n",
      "[73]\tvalid_0's multi_logloss: 0.244856\n",
      "[74]\tvalid_0's multi_logloss: 0.245068\n",
      "[75]\tvalid_0's multi_logloss: 0.245258\n",
      "[76]\tvalid_0's multi_logloss: 0.245262\n",
      "[77]\tvalid_0's multi_logloss: 0.245321\n",
      "[78]\tvalid_0's multi_logloss: 0.245668\n",
      "[79]\tvalid_0's multi_logloss: 0.245454\n",
      "[80]\tvalid_0's multi_logloss: 0.245807\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[81]\tvalid_0's multi_logloss: 0.245871\n",
      "[82]\tvalid_0's multi_logloss: 0.246016\n",
      "[83]\tvalid_0's multi_logloss: 0.246249\n",
      "[84]\tvalid_0's multi_logloss: 0.246317\n",
      "[85]\tvalid_0's multi_logloss: 0.246565\n",
      "[86]\tvalid_0's multi_logloss: 0.246515\n",
      "[87]\tvalid_0's multi_logloss: 0.246853\n",
      "[88]\tvalid_0's multi_logloss: 0.246927\n",
      "[89]\tvalid_0's multi_logloss: 0.247098\n",
      "[90]\tvalid_0's multi_logloss: 0.247229\n",
      "[91]\tvalid_0's multi_logloss: 0.247372\n",
      "[92]\tvalid_0's multi_logloss: 0.247338\n",
      "[93]\tvalid_0's multi_logloss: 0.247314\n",
      "[94]\tvalid_0's multi_logloss: 0.247306\n",
      "[95]\tvalid_0's multi_logloss: 0.247272\n",
      "[96]\tvalid_0's multi_logloss: 0.24753\n",
      "[97]\tvalid_0's multi_logloss: 0.247723\n",
      "[98]\tvalid_0's multi_logloss: 0.24792\n",
      "[99]\tvalid_0's multi_logloss: 0.247992\n",
      "[100]\tvalid_0's multi_logloss: 0.248078\n",
      "[101]\tvalid_0's multi_logloss: 0.248321\n",
      "[102]\tvalid_0's multi_logloss: 0.248448\n",
      "[103]\tvalid_0's multi_logloss: 0.248516\n",
      "[104]\tvalid_0's multi_logloss: 0.248573\n",
      "[105]\tvalid_0's multi_logloss: 0.248789\n",
      "[106]\tvalid_0's multi_logloss: 0.248826\n",
      "[107]\tvalid_0's multi_logloss: 0.249054\n",
      "[108]\tvalid_0's multi_logloss: 0.249456\n",
      "[109]\tvalid_0's multi_logloss: 0.249606\n",
      "[110]\tvalid_0's multi_logloss: 0.249725\n",
      "[111]\tvalid_0's multi_logloss: 0.249924\n",
      "[112]\tvalid_0's multi_logloss: 0.250215\n",
      "[113]\tvalid_0's multi_logloss: 0.250429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[114]\tvalid_0's multi_logloss: 0.250495\n",
      "[115]\tvalid_0's multi_logloss: 0.25071\n",
      "[116]\tvalid_0's multi_logloss: 0.250968\n",
      "[117]\tvalid_0's multi_logloss: 0.250914\n",
      "[118]\tvalid_0's multi_logloss: 0.250991\n",
      "[119]\tvalid_0's multi_logloss: 0.251249\n",
      "[120]\tvalid_0's multi_logloss: 0.251332\n",
      "[121]\tvalid_0's multi_logloss: 0.251566\n",
      "[122]\tvalid_0's multi_logloss: 0.251886\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid_0's multi_logloss: 0.238882\n",
      "lgb now score is: [2.6287929698182824]\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000941 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6532\n",
      "[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 126\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Start training from score -0.062541\n",
      "[LightGBM] [Info] Start training from score -2.803048\n",
      "[1]\tvalid_0's multi_logloss: 0.281946\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's multi_logloss: 0.281844\n",
      "[3]\tvalid_0's multi_logloss: 0.281734\n",
      "[4]\tvalid_0's multi_logloss: 0.281942\n",
      "[5]\tvalid_0's multi_logloss: 0.282222\n",
      "[6]\tvalid_0's multi_logloss: 0.282236\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's multi_logloss: 0.282331\n",
      "[8]\tvalid_0's multi_logloss: 0.282477\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's multi_logloss: 0.283112\n",
      "[10]\tvalid_0's multi_logloss: 0.283282\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's multi_logloss: 0.283718\n",
      "[12]\tvalid_0's multi_logloss: 0.283788\n",
      "[13]\tvalid_0's multi_logloss: 0.283871\n",
      "[14]\tvalid_0's multi_logloss: 0.284377\n",
      "[15]\tvalid_0's multi_logloss: 0.284719\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's multi_logloss: 0.285037\n",
      "[17]\tvalid_0's multi_logloss: 0.285387\n",
      "[18]\tvalid_0's multi_logloss: 0.285297\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's multi_logloss: 0.285433\n",
      "[20]\tvalid_0's multi_logloss: 0.285516\n",
      "[21]\tvalid_0's multi_logloss: 0.285768\n",
      "[22]\tvalid_0's multi_logloss: 0.285878\n",
      "[23]\tvalid_0's multi_logloss: 0.285945\n",
      "[24]\tvalid_0's multi_logloss: 0.286326\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[25]\tvalid_0's multi_logloss: 0.286679\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[26]\tvalid_0's multi_logloss: 0.286843\n",
      "[27]\tvalid_0's multi_logloss: 0.286917\n",
      "[28]\tvalid_0's multi_logloss: 0.287151\n",
      "[29]\tvalid_0's multi_logloss: 0.287561\n",
      "[30]\tvalid_0's multi_logloss: 0.287828\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[31]\tvalid_0's multi_logloss: 0.288061\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[32]\tvalid_0's multi_logloss: 0.288385\n",
      "[33]\tvalid_0's multi_logloss: 0.288453\n",
      "[34]\tvalid_0's multi_logloss: 0.288792\n",
      "[35]\tvalid_0's multi_logloss: 0.288911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[36]\tvalid_0's multi_logloss: 0.289283\n",
      "[37]\tvalid_0's multi_logloss: 0.28937\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[38]\tvalid_0's multi_logloss: 0.28967\n",
      "[39]\tvalid_0's multi_logloss: 0.290189\n",
      "[40]\tvalid_0's multi_logloss: 0.290493\n",
      "[41]\tvalid_0's multi_logloss: 0.290646\n",
      "[42]\tvalid_0's multi_logloss: 0.291136\n",
      "[43]\tvalid_0's multi_logloss: 0.291751\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[44]\tvalid_0's multi_logloss: 0.291952\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[45]\tvalid_0's multi_logloss: 0.292211\n",
      "[46]\tvalid_0's multi_logloss: 0.292941\n",
      "[47]\tvalid_0's multi_logloss: 0.293238\n",
      "[48]\tvalid_0's multi_logloss: 0.293556\n",
      "[49]\tvalid_0's multi_logloss: 0.293628\n",
      "[50]\tvalid_0's multi_logloss: 0.29394\n",
      "[51]\tvalid_0's multi_logloss: 0.29425\n",
      "[52]\tvalid_0's multi_logloss: 0.294516\n",
      "[53]\tvalid_0's multi_logloss: 0.294568\n",
      "[54]\tvalid_0's multi_logloss: 0.295058\n",
      "[55]\tvalid_0's multi_logloss: 0.295461\n",
      "[56]\tvalid_0's multi_logloss: 0.295792\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[57]\tvalid_0's multi_logloss: 0.296059\n",
      "[58]\tvalid_0's multi_logloss: 0.296191\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[59]\tvalid_0's multi_logloss: 0.296894\n",
      "[60]\tvalid_0's multi_logloss: 0.297119\n",
      "[61]\tvalid_0's multi_logloss: 0.297352\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[62]\tvalid_0's multi_logloss: 0.297945\n",
      "[63]\tvalid_0's multi_logloss: 0.298372\n",
      "[64]\tvalid_0's multi_logloss: 0.29898\n",
      "[65]\tvalid_0's multi_logloss: 0.299257\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[66]\tvalid_0's multi_logloss: 0.299394\n",
      "[67]\tvalid_0's multi_logloss: 0.29977\n",
      "[68]\tvalid_0's multi_logloss: 0.300089\n",
      "[69]\tvalid_0's multi_logloss: 0.30065\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[70]\tvalid_0's multi_logloss: 0.300598\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[71]\tvalid_0's multi_logloss: 0.30096\n",
      "[72]\tvalid_0's multi_logloss: 0.30119\n",
      "[73]\tvalid_0's multi_logloss: 0.301337\n",
      "[74]\tvalid_0's multi_logloss: 0.301775\n",
      "[75]\tvalid_0's multi_logloss: 0.301923\n",
      "[76]\tvalid_0's multi_logloss: 0.302121\n",
      "[77]\tvalid_0's multi_logloss: 0.302518\n",
      "[78]\tvalid_0's multi_logloss: 0.302755\n",
      "[79]\tvalid_0's multi_logloss: 0.303029\n",
      "[80]\tvalid_0's multi_logloss: 0.30337\n",
      "[81]\tvalid_0's multi_logloss: 0.303689\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[82]\tvalid_0's multi_logloss: 0.303954\n",
      "[83]\tvalid_0's multi_logloss: 0.304105\n",
      "[84]\tvalid_0's multi_logloss: 0.304428\n",
      "[85]\tvalid_0's multi_logloss: 0.304667\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[86]\tvalid_0's multi_logloss: 0.30503\n",
      "[87]\tvalid_0's multi_logloss: 0.30521\n",
      "[88]\tvalid_0's multi_logloss: 0.305705\n",
      "[89]\tvalid_0's multi_logloss: 0.305934\n",
      "[90]\tvalid_0's multi_logloss: 0.306404\n",
      "[91]\tvalid_0's multi_logloss: 0.306797\n",
      "[92]\tvalid_0's multi_logloss: 0.307295\n",
      "[93]\tvalid_0's multi_logloss: 0.307623\n",
      "[94]\tvalid_0's multi_logloss: 0.308204\n",
      "[95]\tvalid_0's multi_logloss: 0.308508\n",
      "[96]\tvalid_0's multi_logloss: 0.30881\n",
      "[97]\tvalid_0's multi_logloss: 0.309275\n",
      "[98]\tvalid_0's multi_logloss: 0.309364\n",
      "[99]\tvalid_0's multi_logloss: 0.309673\n",
      "[100]\tvalid_0's multi_logloss: 0.310322\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[101]\tvalid_0's multi_logloss: 0.310953\n",
      "[102]\tvalid_0's multi_logloss: 0.31146\n",
      "[103]\tvalid_0's multi_logloss: 0.311671\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's multi_logloss: 0.281734\n",
      "lgb now score is: [2.6287929698182824, 2.5942976577399315]\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000919 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6503\n",
      "[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 125\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Start training from score -0.065205\n",
      "[LightGBM] [Info] Start training from score -2.762638\n",
      "[1]\tvalid_0's multi_logloss: 0.254251\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\tvalid_0's multi_logloss: 0.254478\n",
      "[3]\tvalid_0's multi_logloss: 0.254895\n",
      "[4]\tvalid_0's multi_logloss: 0.255256\n",
      "[5]\tvalid_0's multi_logloss: 0.255521\n",
      "[6]\tvalid_0's multi_logloss: 0.255918\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's multi_logloss: 0.256254\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's multi_logloss: 0.256648\n",
      "[9]\tvalid_0's multi_logloss: 0.256993\n",
      "[10]\tvalid_0's multi_logloss: 0.257344\n",
      "[11]\tvalid_0's multi_logloss: 0.257634\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's multi_logloss: 0.257945\n",
      "[13]\tvalid_0's multi_logloss: 0.258279\n",
      "[14]\tvalid_0's multi_logloss: 0.258484\n",
      "[15]\tvalid_0's multi_logloss: 0.258653\n",
      "[16]\tvalid_0's multi_logloss: 0.25896\n",
      "[17]\tvalid_0's multi_logloss: 0.259147\n",
      "[18]\tvalid_0's multi_logloss: 0.259469\n",
      "[19]\tvalid_0's multi_logloss: 0.259944\n",
      "[20]\tvalid_0's multi_logloss: 0.26022\n",
      "[21]\tvalid_0's multi_logloss: 0.260322\n",
      "[22]\tvalid_0's multi_logloss: 0.26068\n",
      "[23]\tvalid_0's multi_logloss: 0.261082\n",
      "[24]\tvalid_0's multi_logloss: 0.261342\n",
      "[25]\tvalid_0's multi_logloss: 0.261628\n",
      "[26]\tvalid_0's multi_logloss: 0.262088\n",
      "[27]\tvalid_0's multi_logloss: 0.262559\n",
      "[28]\tvalid_0's multi_logloss: 0.262819\n",
      "[29]\tvalid_0's multi_logloss: 0.263179\n",
      "[30]\tvalid_0's multi_logloss: 0.263513\n",
      "[31]\tvalid_0's multi_logloss: 0.263742\n",
      "[32]\tvalid_0's multi_logloss: 0.263755\n",
      "[33]\tvalid_0's multi_logloss: 0.26414\n",
      "[34]\tvalid_0's multi_logloss: 0.264537\n",
      "[35]\tvalid_0's multi_logloss: 0.264828\n",
      "[36]\tvalid_0's multi_logloss: 0.265355\n",
      "[37]\tvalid_0's multi_logloss: 0.265666\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[38]\tvalid_0's multi_logloss: 0.266013\n",
      "[39]\tvalid_0's multi_logloss: 0.266447\n",
      "[40]\tvalid_0's multi_logloss: 0.266663\n",
      "[41]\tvalid_0's multi_logloss: 0.266992\n",
      "[42]\tvalid_0's multi_logloss: 0.267288\n",
      "[43]\tvalid_0's multi_logloss: 0.267666\n",
      "[44]\tvalid_0's multi_logloss: 0.268191\n",
      "[45]\tvalid_0's multi_logloss: 0.268271\n",
      "[46]\tvalid_0's multi_logloss: 0.268383\n",
      "[47]\tvalid_0's multi_logloss: 0.268812\n",
      "[48]\tvalid_0's multi_logloss: 0.269102\n",
      "[49]\tvalid_0's multi_logloss: 0.269458\n",
      "[50]\tvalid_0's multi_logloss: 0.269567\n",
      "[51]\tvalid_0's multi_logloss: 0.269737\n",
      "[52]\tvalid_0's multi_logloss: 0.269986\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[53]\tvalid_0's multi_logloss: 0.270155\n",
      "[54]\tvalid_0's multi_logloss: 0.270656\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[55]\tvalid_0's multi_logloss: 0.270815\n",
      "[56]\tvalid_0's multi_logloss: 0.271181\n",
      "[57]\tvalid_0's multi_logloss: 0.27147\n",
      "[58]\tvalid_0's multi_logloss: 0.2716\n",
      "[59]\tvalid_0's multi_logloss: 0.271873\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[60]\tvalid_0's multi_logloss: 0.27211\n",
      "[61]\tvalid_0's multi_logloss: 0.272254\n",
      "[62]\tvalid_0's multi_logloss: 0.272509\n",
      "[63]\tvalid_0's multi_logloss: 0.272898\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[64]\tvalid_0's multi_logloss: 0.273406\n",
      "[65]\tvalid_0's multi_logloss: 0.27367\n",
      "[66]\tvalid_0's multi_logloss: 0.274\n",
      "[67]\tvalid_0's multi_logloss: 0.274398\n",
      "[68]\tvalid_0's multi_logloss: 0.274511\n",
      "[69]\tvalid_0's multi_logloss: 0.27484\n",
      "[70]\tvalid_0's multi_logloss: 0.275066\n",
      "[71]\tvalid_0's multi_logloss: 0.275283\n",
      "[72]\tvalid_0's multi_logloss: 0.275642\n",
      "[73]\tvalid_0's multi_logloss: 0.276023\n",
      "[74]\tvalid_0's multi_logloss: 0.276149\n",
      "[75]\tvalid_0's multi_logloss: 0.276427\n",
      "[76]\tvalid_0's multi_logloss: 0.276783\n",
      "[77]\tvalid_0's multi_logloss: 0.277128\n",
      "[78]\tvalid_0's multi_logloss: 0.277526\n",
      "[79]\tvalid_0's multi_logloss: 0.278042\n",
      "[80]\tvalid_0's multi_logloss: 0.278197\n",
      "[81]\tvalid_0's multi_logloss: 0.278618\n",
      "[82]\tvalid_0's multi_logloss: 0.278842\n",
      "[83]\tvalid_0's multi_logloss: 0.279161\n",
      "[84]\tvalid_0's multi_logloss: 0.279458\n",
      "[85]\tvalid_0's multi_logloss: 0.279796\n",
      "[86]\tvalid_0's multi_logloss: 0.279991\n",
      "[87]\tvalid_0's multi_logloss: 0.280131\n",
      "[88]\tvalid_0's multi_logloss: 0.280308\n",
      "[89]\tvalid_0's multi_logloss: 0.280665\n",
      "[90]\tvalid_0's multi_logloss: 0.281019\n",
      "[91]\tvalid_0's multi_logloss: 0.281204\n",
      "[92]\tvalid_0's multi_logloss: 0.281434\n",
      "[93]\tvalid_0's multi_logloss: 0.281652\n",
      "[94]\tvalid_0's multi_logloss: 0.282138\n",
      "[95]\tvalid_0's multi_logloss: 0.282369\n",
      "[96]\tvalid_0's multi_logloss: 0.28273\n",
      "[97]\tvalid_0's multi_logloss: 0.283107\n",
      "[98]\tvalid_0's multi_logloss: 0.283467\n",
      "[99]\tvalid_0's multi_logloss: 0.283723\n",
      "[100]\tvalid_0's multi_logloss: 0.283975\n",
      "[101]\tvalid_0's multi_logloss: 0.284325\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's multi_logloss: 0.254251\n",
      "lgb now score is: [2.6287929698182824, 2.5942976577399315, 2.5791322680805386]\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001079 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6482\n",
      "[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 125\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Start training from score -0.069886\n",
      "[LightGBM] [Info] Start training from score -2.695628\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's multi_logloss: 0.207448\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\tvalid_0's multi_logloss: 0.207101\n",
      "[3]\tvalid_0's multi_logloss: 0.206753\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's multi_logloss: 0.206501\n",
      "[5]\tvalid_0's multi_logloss: 0.206091\n",
      "[6]\tvalid_0's multi_logloss: 0.205794\n",
      "[7]\tvalid_0's multi_logloss: 0.205559\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's multi_logloss: 0.205432\n",
      "[9]\tvalid_0's multi_logloss: 0.205525\n",
      "[10]\tvalid_0's multi_logloss: 0.205501\n",
      "[11]\tvalid_0's multi_logloss: 0.205269\n",
      "[12]\tvalid_0's multi_logloss: 0.205131\n",
      "[13]\tvalid_0's multi_logloss: 0.204921\n",
      "[14]\tvalid_0's multi_logloss: 0.204696\n",
      "[15]\tvalid_0's multi_logloss: 0.204491\n",
      "[16]\tvalid_0's multi_logloss: 0.204296\n",
      "[17]\tvalid_0's multi_logloss: 0.204116\n",
      "[18]\tvalid_0's multi_logloss: 0.204113\n",
      "[19]\tvalid_0's multi_logloss: 0.203863\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's multi_logloss: 0.203724\n",
      "[21]\tvalid_0's multi_logloss: 0.203619\n",
      "[22]\tvalid_0's multi_logloss: 0.203587\n",
      "[23]\tvalid_0's multi_logloss: 0.203622\n",
      "[24]\tvalid_0's multi_logloss: 0.203505\n",
      "[25]\tvalid_0's multi_logloss: 0.203151\n",
      "[26]\tvalid_0's multi_logloss: 0.203279\n",
      "[27]\tvalid_0's multi_logloss: 0.203308\n",
      "[28]\tvalid_0's multi_logloss: 0.203229\n",
      "[29]\tvalid_0's multi_logloss: 0.203355\n",
      "[30]\tvalid_0's multi_logloss: 0.20343\n",
      "[31]\tvalid_0's multi_logloss: 0.203209\n",
      "[32]\tvalid_0's multi_logloss: 0.203483\n",
      "[33]\tvalid_0's multi_logloss: 0.203658\n",
      "[34]\tvalid_0's multi_logloss: 0.203805\n",
      "[35]\tvalid_0's multi_logloss: 0.203961\n",
      "[36]\tvalid_0's multi_logloss: 0.204181\n",
      "[37]\tvalid_0's multi_logloss: 0.204143\n",
      "[38]\tvalid_0's multi_logloss: 0.203999\n",
      "[39]\tvalid_0's multi_logloss: 0.204101\n",
      "[40]\tvalid_0's multi_logloss: 0.204356\n",
      "[41]\tvalid_0's multi_logloss: 0.204454\n",
      "[42]\tvalid_0's multi_logloss: 0.204339\n",
      "[43]\tvalid_0's multi_logloss: 0.20454\n",
      "[44]\tvalid_0's multi_logloss: 0.204786\n",
      "[45]\tvalid_0's multi_logloss: 0.205089\n",
      "[46]\tvalid_0's multi_logloss: 0.205145\n",
      "[47]\tvalid_0's multi_logloss: 0.205176\n",
      "[48]\tvalid_0's multi_logloss: 0.205437\n",
      "[49]\tvalid_0's multi_logloss: 0.205442\n",
      "[50]\tvalid_0's multi_logloss: 0.205492\n",
      "[51]\tvalid_0's multi_logloss: 0.205617\n",
      "[52]\tvalid_0's multi_logloss: 0.20577\n",
      "[53]\tvalid_0's multi_logloss: 0.205778\n",
      "[54]\tvalid_0's multi_logloss: 0.205891\n",
      "[55]\tvalid_0's multi_logloss: 0.20598\n",
      "[56]\tvalid_0's multi_logloss: 0.205872\n",
      "[57]\tvalid_0's multi_logloss: 0.2058\n",
      "[58]\tvalid_0's multi_logloss: 0.205898\n",
      "[59]\tvalid_0's multi_logloss: 0.206069\n",
      "[60]\tvalid_0's multi_logloss: 0.206143\n",
      "[61]\tvalid_0's multi_logloss: 0.206251\n",
      "[62]\tvalid_0's multi_logloss: 0.206407\n",
      "[63]\tvalid_0's multi_logloss: 0.206696\n",
      "[64]\tvalid_0's multi_logloss: 0.206803\n",
      "[65]\tvalid_0's multi_logloss: 0.206889\n",
      "[66]\tvalid_0's multi_logloss: 0.206927\n",
      "[67]\tvalid_0's multi_logloss: 0.206992\n",
      "[68]\tvalid_0's multi_logloss: 0.207065\n",
      "[69]\tvalid_0's multi_logloss: 0.207215\n",
      "[70]\tvalid_0's multi_logloss: 0.207215\n",
      "[71]\tvalid_0's multi_logloss: 0.20721\n",
      "[72]\tvalid_0's multi_logloss: 0.207363\n",
      "[73]\tvalid_0's multi_logloss: 0.207408\n",
      "[74]\tvalid_0's multi_logloss: 0.207571\n",
      "[75]\tvalid_0's multi_logloss: 0.207553\n",
      "[76]\tvalid_0's multi_logloss: 0.207472\n",
      "[77]\tvalid_0's multi_logloss: 0.207661\n",
      "[78]\tvalid_0's multi_logloss: 0.207859\n",
      "[79]\tvalid_0's multi_logloss: 0.208131\n",
      "[80]\tvalid_0's multi_logloss: 0.208309\n",
      "[81]\tvalid_0's multi_logloss: 0.208409\n",
      "[82]\tvalid_0's multi_logloss: 0.208446\n",
      "[83]\tvalid_0's multi_logloss: 0.208513\n",
      "[84]\tvalid_0's multi_logloss: 0.20846\n",
      "[85]\tvalid_0's multi_logloss: 0.208442\n",
      "[86]\tvalid_0's multi_logloss: 0.208455\n",
      "[87]\tvalid_0's multi_logloss: 0.208701\n",
      "[88]\tvalid_0's multi_logloss: 0.208839\n",
      "[89]\tvalid_0's multi_logloss: 0.208964\n",
      "[90]\tvalid_0's multi_logloss: 0.208926\n",
      "[91]\tvalid_0's multi_logloss: 0.208874\n",
      "[92]\tvalid_0's multi_logloss: 0.208837\n",
      "[93]\tvalid_0's multi_logloss: 0.20877\n",
      "[94]\tvalid_0's multi_logloss: 0.208933\n",
      "[95]\tvalid_0's multi_logloss: 0.208847\n",
      "[96]\tvalid_0's multi_logloss: 0.208817\n",
      "[97]\tvalid_0's multi_logloss: 0.209091\n",
      "[98]\tvalid_0's multi_logloss: 0.209144\n",
      "[99]\tvalid_0's multi_logloss: 0.209103\n",
      "[100]\tvalid_0's multi_logloss: 0.209177\n",
      "[101]\tvalid_0's multi_logloss: 0.209378\n",
      "[102]\tvalid_0's multi_logloss: 0.209533\n",
      "[103]\tvalid_0's multi_logloss: 0.209569\n",
      "[104]\tvalid_0's multi_logloss: 0.209803\n",
      "[105]\tvalid_0's multi_logloss: 0.209969\n",
      "[106]\tvalid_0's multi_logloss: 0.210268\n",
      "[107]\tvalid_0's multi_logloss: 0.2103\n",
      "[108]\tvalid_0's multi_logloss: 0.210385\n",
      "[109]\tvalid_0's multi_logloss: 0.210639\n",
      "[110]\tvalid_0's multi_logloss: 0.210918\n",
      "[111]\tvalid_0's multi_logloss: 0.211002\n",
      "[112]\tvalid_0's multi_logloss: 0.211125\n",
      "[113]\tvalid_0's multi_logloss: 0.211374\n",
      "[114]\tvalid_0's multi_logloss: 0.211409\n",
      "[115]\tvalid_0's multi_logloss: 0.211657\n",
      "[116]\tvalid_0's multi_logloss: 0.212008\n",
      "[117]\tvalid_0's multi_logloss: 0.212138\n",
      "[118]\tvalid_0's multi_logloss: 0.212096\n",
      "[119]\tvalid_0's multi_logloss: 0.212244\n",
      "[120]\tvalid_0's multi_logloss: 0.212452\n",
      "[121]\tvalid_0's multi_logloss: 0.21245\n",
      "[122]\tvalid_0's multi_logloss: 0.212749\n",
      "[123]\tvalid_0's multi_logloss: 0.21291\n",
      "[124]\tvalid_0's multi_logloss: 0.213156\n",
      "[125]\tvalid_0's multi_logloss: 0.21324\n",
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's multi_logloss: 0.203151\n",
      "lgb now score is: [2.6287929698182824, 2.5942976577399315, 2.5791322680805386, 2.6414094350732307]\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001064 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6467\n",
      "[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 125\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Start training from score -0.069216\n",
      "[LightGBM] [Info] Start training from score -2.704930\n",
      "[1]\tvalid_0's multi_logloss: 0.213879\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\tvalid_0's multi_logloss: 0.213844\n",
      "[3]\tvalid_0's multi_logloss: 0.213664\n",
      "[4]\tvalid_0's multi_logloss: 0.213446\n",
      "[5]\tvalid_0's multi_logloss: 0.213363\n",
      "[6]\tvalid_0's multi_logloss: 0.213531\n",
      "[7]\tvalid_0's multi_logloss: 0.21339\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's multi_logloss: 0.213383\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's multi_logloss: 0.213238\n",
      "[10]\tvalid_0's multi_logloss: 0.213098\n",
      "[11]\tvalid_0's multi_logloss: 0.212924\n",
      "[12]\tvalid_0's multi_logloss: 0.212535\n",
      "[13]\tvalid_0's multi_logloss: 0.212486\n",
      "[14]\tvalid_0's multi_logloss: 0.212499\n",
      "[15]\tvalid_0's multi_logloss: 0.212252\n",
      "[16]\tvalid_0's multi_logloss: 0.212204\n",
      "[17]\tvalid_0's multi_logloss: 0.212256\n",
      "[18]\tvalid_0's multi_logloss: 0.212181\n",
      "[19]\tvalid_0's multi_logloss: 0.212083\n",
      "[20]\tvalid_0's multi_logloss: 0.211923\n",
      "[21]\tvalid_0's multi_logloss: 0.21197\n",
      "[22]\tvalid_0's multi_logloss: 0.212034\n",
      "[23]\tvalid_0's multi_logloss: 0.211985\n",
      "[24]\tvalid_0's multi_logloss: 0.211912\n",
      "[25]\tvalid_0's multi_logloss: 0.211854\n",
      "[26]\tvalid_0's multi_logloss: 0.211865\n",
      "[27]\tvalid_0's multi_logloss: 0.211883\n",
      "[28]\tvalid_0's multi_logloss: 0.211887\n",
      "[29]\tvalid_0's multi_logloss: 0.211959\n",
      "[30]\tvalid_0's multi_logloss: 0.212061\n",
      "[31]\tvalid_0's multi_logloss: 0.212005\n",
      "[32]\tvalid_0's multi_logloss: 0.212211\n",
      "[33]\tvalid_0's multi_logloss: 0.212243\n",
      "[34]\tvalid_0's multi_logloss: 0.212186\n",
      "[35]\tvalid_0's multi_logloss: 0.211975\n",
      "[36]\tvalid_0's multi_logloss: 0.212051\n",
      "[37]\tvalid_0's multi_logloss: 0.212076\n",
      "[38]\tvalid_0's multi_logloss: 0.212198\n",
      "[39]\tvalid_0's multi_logloss: 0.212252\n",
      "[40]\tvalid_0's multi_logloss: 0.212363\n",
      "[41]\tvalid_0's multi_logloss: 0.212507\n",
      "[42]\tvalid_0's multi_logloss: 0.212642\n",
      "[43]\tvalid_0's multi_logloss: 0.212471\n",
      "[44]\tvalid_0's multi_logloss: 0.212713\n",
      "[45]\tvalid_0's multi_logloss: 0.212837\n",
      "[46]\tvalid_0's multi_logloss: 0.213131\n",
      "[47]\tvalid_0's multi_logloss: 0.213025\n",
      "[48]\tvalid_0's multi_logloss: 0.212839\n",
      "[49]\tvalid_0's multi_logloss: 0.212983\n",
      "[50]\tvalid_0's multi_logloss: 0.212969\n",
      "[51]\tvalid_0's multi_logloss: 0.213071\n",
      "[52]\tvalid_0's multi_logloss: 0.21324\n",
      "[53]\tvalid_0's multi_logloss: 0.213518\n",
      "[54]\tvalid_0's multi_logloss: 0.21335\n",
      "[55]\tvalid_0's multi_logloss: 0.2134\n",
      "[56]\tvalid_0's multi_logloss: 0.213472\n",
      "[57]\tvalid_0's multi_logloss: 0.213624\n",
      "[58]\tvalid_0's multi_logloss: 0.213881\n",
      "[59]\tvalid_0's multi_logloss: 0.214063\n",
      "[60]\tvalid_0's multi_logloss: 0.214243\n",
      "[61]\tvalid_0's multi_logloss: 0.214407\n",
      "[62]\tvalid_0's multi_logloss: 0.214652\n",
      "[63]\tvalid_0's multi_logloss: 0.214637\n",
      "[64]\tvalid_0's multi_logloss: 0.214876\n",
      "[65]\tvalid_0's multi_logloss: 0.214914\n",
      "[66]\tvalid_0's multi_logloss: 0.21497\n",
      "[67]\tvalid_0's multi_logloss: 0.215109\n",
      "[68]\tvalid_0's multi_logloss: 0.215176\n",
      "[69]\tvalid_0's multi_logloss: 0.215031\n",
      "[70]\tvalid_0's multi_logloss: 0.215464\n",
      "[71]\tvalid_0's multi_logloss: 0.215564\n",
      "[72]\tvalid_0's multi_logloss: 0.21582\n",
      "[73]\tvalid_0's multi_logloss: 0.216013\n",
      "[74]\tvalid_0's multi_logloss: 0.216115\n",
      "[75]\tvalid_0's multi_logloss: 0.216191\n",
      "[76]\tvalid_0's multi_logloss: 0.21629\n",
      "[77]\tvalid_0's multi_logloss: 0.216337\n",
      "[78]\tvalid_0's multi_logloss: 0.21647\n",
      "[79]\tvalid_0's multi_logloss: 0.216641\n",
      "[80]\tvalid_0's multi_logloss: 0.216561\n",
      "[81]\tvalid_0's multi_logloss: 0.216753\n",
      "[82]\tvalid_0's multi_logloss: 0.216923\n",
      "[83]\tvalid_0's multi_logloss: 0.217101\n",
      "[84]\tvalid_0's multi_logloss: 0.217148\n",
      "[85]\tvalid_0's multi_logloss: 0.217499\n",
      "[86]\tvalid_0's multi_logloss: 0.21758\n",
      "[87]\tvalid_0's multi_logloss: 0.217785\n",
      "[88]\tvalid_0's multi_logloss: 0.218022\n",
      "[89]\tvalid_0's multi_logloss: 0.21825\n",
      "[90]\tvalid_0's multi_logloss: 0.218454\n",
      "[91]\tvalid_0's multi_logloss: 0.218523\n",
      "[92]\tvalid_0's multi_logloss: 0.218612\n",
      "[93]\tvalid_0's multi_logloss: 0.218786\n",
      "[94]\tvalid_0's multi_logloss: 0.219008\n",
      "[95]\tvalid_0's multi_logloss: 0.219222\n",
      "[96]\tvalid_0's multi_logloss: 0.219371\n",
      "[97]\tvalid_0's multi_logloss: 0.219406\n",
      "[98]\tvalid_0's multi_logloss: 0.219499\n",
      "[99]\tvalid_0's multi_logloss: 0.219581\n",
      "[100]\tvalid_0's multi_logloss: 0.21977\n",
      "[101]\tvalid_0's multi_logloss: 0.219834\n",
      "[102]\tvalid_0's multi_logloss: 0.219885\n",
      "[103]\tvalid_0's multi_logloss: 0.220173\n",
      "[104]\tvalid_0's multi_logloss: 0.220459\n",
      "[105]\tvalid_0's multi_logloss: 0.220789\n",
      "[106]\tvalid_0's multi_logloss: 0.221046\n",
      "[107]\tvalid_0's multi_logloss: 0.221371\n",
      "[108]\tvalid_0's multi_logloss: 0.221584\n",
      "[109]\tvalid_0's multi_logloss: 0.221616\n",
      "[110]\tvalid_0's multi_logloss: 0.221738\n",
      "[111]\tvalid_0's multi_logloss: 0.221975\n",
      "[112]\tvalid_0's multi_logloss: 0.222298\n",
      "[113]\tvalid_0's multi_logloss: 0.222499\n",
      "[114]\tvalid_0's multi_logloss: 0.222706\n",
      "[115]\tvalid_0's multi_logloss: 0.222831\n",
      "[116]\tvalid_0's multi_logloss: 0.22317\n",
      "[117]\tvalid_0's multi_logloss: 0.223348\n",
      "[118]\tvalid_0's multi_logloss: 0.223532\n",
      "[119]\tvalid_0's multi_logloss: 0.223689\n",
      "[120]\tvalid_0's multi_logloss: 0.223952\n",
      "[121]\tvalid_0's multi_logloss: 0.22409\n",
      "[122]\tvalid_0's multi_logloss: 0.22417\n",
      "[123]\tvalid_0's multi_logloss: 0.224385\n",
      "[124]\tvalid_0's multi_logloss: 0.224575\n",
      "[125]\tvalid_0's multi_logloss: 0.2247\n",
      "Early stopping, best iteration is:\n",
      "[25]\tvalid_0's multi_logloss: 0.211854\n",
      "lgb now score is: [2.6287929698182824, 2.5942976577399315, 2.5791322680805386, 2.6414094350732307, 2.6558220675886015]\n",
      "lgb_score_list: [2.6287929698182824, 2.5942976577399315, 2.5791322680805386, 2.6414094350732307, 2.6558220675886015]\n",
      "lgb_score_mean: 2.619890879660117\n",
      "[0]\ttrain-mlogloss:0.67092\teval-mlogloss:0.67101\n",
      "[1]\ttrain-mlogloss:0.65009\teval-mlogloss:0.65047\n",
      "[2]\ttrain-mlogloss:0.63051\teval-mlogloss:0.63136\n",
      "[3]\ttrain-mlogloss:0.61200\teval-mlogloss:0.61282\n",
      "[4]\ttrain-mlogloss:0.59438\teval-mlogloss:0.59526\n",
      "[5]\ttrain-mlogloss:0.57765\teval-mlogloss:0.57864\n",
      "[6]\ttrain-mlogloss:0.56190\teval-mlogloss:0.56320\n",
      "[7]\ttrain-mlogloss:0.54668\teval-mlogloss:0.54829\n",
      "[8]\ttrain-mlogloss:0.53251\teval-mlogloss:0.53451\n",
      "[9]\ttrain-mlogloss:0.51867\teval-mlogloss:0.52102\n",
      "[10]\ttrain-mlogloss:0.50572\teval-mlogloss:0.50836\n",
      "[11]\ttrain-mlogloss:0.49336\teval-mlogloss:0.49637\n",
      "[12]\ttrain-mlogloss:0.48172\teval-mlogloss:0.48474\n",
      "[13]\ttrain-mlogloss:0.47056\teval-mlogloss:0.47415\n",
      "[14]\ttrain-mlogloss:0.45982\teval-mlogloss:0.46348\n",
      "[15]\ttrain-mlogloss:0.44989\teval-mlogloss:0.45387\n",
      "[16]\ttrain-mlogloss:0.44033\teval-mlogloss:0.44439\n",
      "[17]\ttrain-mlogloss:0.43110\teval-mlogloss:0.43534\n",
      "[18]\ttrain-mlogloss:0.42227\teval-mlogloss:0.42678\n",
      "[19]\ttrain-mlogloss:0.41369\teval-mlogloss:0.41834\n",
      "[20]\ttrain-mlogloss:0.40549\teval-mlogloss:0.41024\n",
      "[21]\ttrain-mlogloss:0.39792\teval-mlogloss:0.40277\n",
      "[22]\ttrain-mlogloss:0.39038\teval-mlogloss:0.39546\n",
      "[23]\ttrain-mlogloss:0.38338\teval-mlogloss:0.38856\n",
      "[24]\ttrain-mlogloss:0.37664\teval-mlogloss:0.38199\n",
      "[25]\ttrain-mlogloss:0.37024\teval-mlogloss:0.37600\n",
      "[26]\ttrain-mlogloss:0.36393\teval-mlogloss:0.37009\n",
      "[27]\ttrain-mlogloss:0.35798\teval-mlogloss:0.36443\n",
      "[28]\ttrain-mlogloss:0.35225\teval-mlogloss:0.35897\n",
      "[29]\ttrain-mlogloss:0.34673\teval-mlogloss:0.35367\n",
      "[30]\ttrain-mlogloss:0.34149\teval-mlogloss:0.34855\n",
      "[31]\ttrain-mlogloss:0.33651\teval-mlogloss:0.34378\n",
      "[32]\ttrain-mlogloss:0.33170\teval-mlogloss:0.33934\n",
      "[33]\ttrain-mlogloss:0.32712\teval-mlogloss:0.33492\n",
      "[34]\ttrain-mlogloss:0.32259\teval-mlogloss:0.33071\n",
      "[35]\ttrain-mlogloss:0.31833\teval-mlogloss:0.32676\n",
      "[36]\ttrain-mlogloss:0.31424\teval-mlogloss:0.32293\n",
      "[37]\ttrain-mlogloss:0.31030\teval-mlogloss:0.31935\n",
      "[38]\ttrain-mlogloss:0.30646\teval-mlogloss:0.31566\n",
      "[39]\ttrain-mlogloss:0.30288\teval-mlogloss:0.31234\n",
      "[40]\ttrain-mlogloss:0.29929\teval-mlogloss:0.30880\n",
      "[41]\ttrain-mlogloss:0.29606\teval-mlogloss:0.30567\n",
      "[42]\ttrain-mlogloss:0.29278\teval-mlogloss:0.30278\n",
      "[43]\ttrain-mlogloss:0.28957\teval-mlogloss:0.29993\n",
      "[44]\ttrain-mlogloss:0.28659\teval-mlogloss:0.29723\n",
      "[45]\ttrain-mlogloss:0.28343\teval-mlogloss:0.29443\n",
      "[46]\ttrain-mlogloss:0.28066\teval-mlogloss:0.29213\n",
      "[47]\ttrain-mlogloss:0.27813\teval-mlogloss:0.28987\n",
      "[48]\ttrain-mlogloss:0.27558\teval-mlogloss:0.28750\n",
      "[49]\ttrain-mlogloss:0.27316\teval-mlogloss:0.28525\n",
      "[50]\ttrain-mlogloss:0.27067\teval-mlogloss:0.28302\n",
      "[51]\ttrain-mlogloss:0.26850\teval-mlogloss:0.28109\n",
      "[52]\ttrain-mlogloss:0.26642\teval-mlogloss:0.27931\n",
      "[53]\ttrain-mlogloss:0.26419\teval-mlogloss:0.27726\n",
      "[54]\ttrain-mlogloss:0.26221\teval-mlogloss:0.27538\n",
      "[55]\ttrain-mlogloss:0.26038\teval-mlogloss:0.27371\n",
      "[56]\ttrain-mlogloss:0.25849\teval-mlogloss:0.27202\n",
      "[57]\ttrain-mlogloss:0.25674\teval-mlogloss:0.27049\n",
      "[58]\ttrain-mlogloss:0.25499\teval-mlogloss:0.26896\n",
      "[59]\ttrain-mlogloss:0.25329\teval-mlogloss:0.26748\n",
      "[60]\ttrain-mlogloss:0.25159\teval-mlogloss:0.26597\n",
      "[61]\ttrain-mlogloss:0.25002\teval-mlogloss:0.26474\n",
      "[62]\ttrain-mlogloss:0.24857\teval-mlogloss:0.26343\n",
      "[63]\ttrain-mlogloss:0.24720\teval-mlogloss:0.26223\n",
      "[64]\ttrain-mlogloss:0.24580\teval-mlogloss:0.26114\n",
      "[65]\ttrain-mlogloss:0.24432\teval-mlogloss:0.25992\n",
      "[66]\ttrain-mlogloss:0.24292\teval-mlogloss:0.25883\n",
      "[67]\ttrain-mlogloss:0.24164\teval-mlogloss:0.25786\n",
      "[68]\ttrain-mlogloss:0.24036\teval-mlogloss:0.25714\n",
      "[69]\ttrain-mlogloss:0.23901\teval-mlogloss:0.25610\n",
      "[70]\ttrain-mlogloss:0.23791\teval-mlogloss:0.25515\n",
      "[71]\ttrain-mlogloss:0.23691\teval-mlogloss:0.25437\n",
      "[72]\ttrain-mlogloss:0.23575\teval-mlogloss:0.25358\n",
      "[73]\ttrain-mlogloss:0.23465\teval-mlogloss:0.25272\n",
      "[74]\ttrain-mlogloss:0.23359\teval-mlogloss:0.25200\n",
      "[75]\ttrain-mlogloss:0.23238\teval-mlogloss:0.25122\n",
      "[76]\ttrain-mlogloss:0.23149\teval-mlogloss:0.25064\n",
      "[77]\ttrain-mlogloss:0.23039\teval-mlogloss:0.24999\n",
      "[78]\ttrain-mlogloss:0.22948\teval-mlogloss:0.24935\n",
      "[79]\ttrain-mlogloss:0.22865\teval-mlogloss:0.24875\n",
      "[80]\ttrain-mlogloss:0.22781\teval-mlogloss:0.24821\n",
      "[81]\ttrain-mlogloss:0.22695\teval-mlogloss:0.24767\n",
      "[82]\ttrain-mlogloss:0.22609\teval-mlogloss:0.24728\n",
      "[83]\ttrain-mlogloss:0.22517\teval-mlogloss:0.24688\n",
      "[84]\ttrain-mlogloss:0.22435\teval-mlogloss:0.24657\n",
      "[85]\ttrain-mlogloss:0.22348\teval-mlogloss:0.24618\n",
      "[86]\ttrain-mlogloss:0.22271\teval-mlogloss:0.24565\n",
      "[87]\ttrain-mlogloss:0.22204\teval-mlogloss:0.24543\n",
      "[88]\ttrain-mlogloss:0.22128\teval-mlogloss:0.24510\n",
      "[89]\ttrain-mlogloss:0.22053\teval-mlogloss:0.24488\n",
      "[90]\ttrain-mlogloss:0.21979\teval-mlogloss:0.24454\n",
      "[91]\ttrain-mlogloss:0.21900\teval-mlogloss:0.24426\n",
      "[92]\ttrain-mlogloss:0.21818\teval-mlogloss:0.24383\n",
      "[93]\ttrain-mlogloss:0.21764\teval-mlogloss:0.24359\n",
      "[94]\ttrain-mlogloss:0.21695\teval-mlogloss:0.24336\n",
      "[95]\ttrain-mlogloss:0.21625\teval-mlogloss:0.24316\n",
      "[96]\ttrain-mlogloss:0.21569\teval-mlogloss:0.24293\n",
      "[97]\ttrain-mlogloss:0.21505\teval-mlogloss:0.24272\n",
      "[98]\ttrain-mlogloss:0.21443\teval-mlogloss:0.24265\n",
      "[99]\ttrain-mlogloss:0.21371\teval-mlogloss:0.24255\n",
      "[100]\ttrain-mlogloss:0.21313\teval-mlogloss:0.24239\n",
      "[101]\ttrain-mlogloss:0.21255\teval-mlogloss:0.24222\n",
      "[102]\ttrain-mlogloss:0.21188\teval-mlogloss:0.24209\n",
      "[103]\ttrain-mlogloss:0.21118\teval-mlogloss:0.24185\n",
      "[104]\ttrain-mlogloss:0.21059\teval-mlogloss:0.24183\n",
      "[105]\ttrain-mlogloss:0.20997\teval-mlogloss:0.24189\n",
      "[106]\ttrain-mlogloss:0.20937\teval-mlogloss:0.24193\n",
      "[107]\ttrain-mlogloss:0.20867\teval-mlogloss:0.24187\n",
      "[108]\ttrain-mlogloss:0.20815\teval-mlogloss:0.24178\n",
      "[109]\ttrain-mlogloss:0.20765\teval-mlogloss:0.24155\n",
      "[110]\ttrain-mlogloss:0.20706\teval-mlogloss:0.24150\n",
      "[111]\ttrain-mlogloss:0.20648\teval-mlogloss:0.24142\n",
      "[112]\ttrain-mlogloss:0.20589\teval-mlogloss:0.24137\n",
      "[113]\ttrain-mlogloss:0.20540\teval-mlogloss:0.24143\n",
      "[114]\ttrain-mlogloss:0.20489\teval-mlogloss:0.24138\n",
      "[115]\ttrain-mlogloss:0.20421\teval-mlogloss:0.24143\n",
      "[116]\ttrain-mlogloss:0.20365\teval-mlogloss:0.24149\n",
      "[117]\ttrain-mlogloss:0.20326\teval-mlogloss:0.24131\n",
      "[118]\ttrain-mlogloss:0.20271\teval-mlogloss:0.24133\n",
      "[119]\ttrain-mlogloss:0.20225\teval-mlogloss:0.24125\n",
      "[120]\ttrain-mlogloss:0.20175\teval-mlogloss:0.24113\n",
      "[121]\ttrain-mlogloss:0.20130\teval-mlogloss:0.24117\n",
      "[122]\ttrain-mlogloss:0.20083\teval-mlogloss:0.24103\n",
      "[123]\ttrain-mlogloss:0.20025\teval-mlogloss:0.24107\n",
      "[124]\ttrain-mlogloss:0.19973\teval-mlogloss:0.24120\n",
      "[125]\ttrain-mlogloss:0.19937\teval-mlogloss:0.24102\n",
      "[126]\ttrain-mlogloss:0.19897\teval-mlogloss:0.24087\n",
      "[127]\ttrain-mlogloss:0.19848\teval-mlogloss:0.24108\n",
      "[128]\ttrain-mlogloss:0.19808\teval-mlogloss:0.24102\n",
      "[129]\ttrain-mlogloss:0.19762\teval-mlogloss:0.24115\n",
      "[130]\ttrain-mlogloss:0.19716\teval-mlogloss:0.24127\n",
      "[131]\ttrain-mlogloss:0.19674\teval-mlogloss:0.24148\n",
      "[132]\ttrain-mlogloss:0.19619\teval-mlogloss:0.24151\n",
      "[133]\ttrain-mlogloss:0.19580\teval-mlogloss:0.24163\n",
      "[134]\ttrain-mlogloss:0.19546\teval-mlogloss:0.24163\n",
      "[135]\ttrain-mlogloss:0.19490\teval-mlogloss:0.24166\n",
      "[136]\ttrain-mlogloss:0.19452\teval-mlogloss:0.24147\n",
      "[137]\ttrain-mlogloss:0.19418\teval-mlogloss:0.24164\n",
      "[138]\ttrain-mlogloss:0.19358\teval-mlogloss:0.24174\n",
      "[139]\ttrain-mlogloss:0.19330\teval-mlogloss:0.24175\n",
      "[140]\ttrain-mlogloss:0.19295\teval-mlogloss:0.24171\n",
      "[141]\ttrain-mlogloss:0.19247\teval-mlogloss:0.24170\n",
      "[142]\ttrain-mlogloss:0.19214\teval-mlogloss:0.24171\n",
      "[143]\ttrain-mlogloss:0.19151\teval-mlogloss:0.24216\n",
      "[144]\ttrain-mlogloss:0.19114\teval-mlogloss:0.24235\n",
      "[145]\ttrain-mlogloss:0.19072\teval-mlogloss:0.24246\n",
      "[146]\ttrain-mlogloss:0.19034\teval-mlogloss:0.24249\n",
      "[147]\ttrain-mlogloss:0.18998\teval-mlogloss:0.24256\n",
      "[148]\ttrain-mlogloss:0.18976\teval-mlogloss:0.24240\n",
      "[149]\ttrain-mlogloss:0.18937\teval-mlogloss:0.24263\n",
      "[150]\ttrain-mlogloss:0.18903\teval-mlogloss:0.24265\n",
      "[151]\ttrain-mlogloss:0.18862\teval-mlogloss:0.24280\n",
      "[152]\ttrain-mlogloss:0.18827\teval-mlogloss:0.24292\n",
      "[153]\ttrain-mlogloss:0.18782\teval-mlogloss:0.24285\n",
      "[154]\ttrain-mlogloss:0.18752\teval-mlogloss:0.24295\n",
      "[155]\ttrain-mlogloss:0.18705\teval-mlogloss:0.24286\n",
      "[156]\ttrain-mlogloss:0.18669\teval-mlogloss:0.24266\n",
      "[157]\ttrain-mlogloss:0.18640\teval-mlogloss:0.24264\n",
      "[158]\ttrain-mlogloss:0.18604\teval-mlogloss:0.24284\n",
      "[159]\ttrain-mlogloss:0.18575\teval-mlogloss:0.24296\n",
      "[160]\ttrain-mlogloss:0.18545\teval-mlogloss:0.24288\n",
      "[161]\ttrain-mlogloss:0.18509\teval-mlogloss:0.24292\n",
      "[162]\ttrain-mlogloss:0.18466\teval-mlogloss:0.24295\n",
      "[163]\ttrain-mlogloss:0.18432\teval-mlogloss:0.24296\n",
      "[164]\ttrain-mlogloss:0.18402\teval-mlogloss:0.24289\n",
      "[165]\ttrain-mlogloss:0.18367\teval-mlogloss:0.24283\n",
      "[166]\ttrain-mlogloss:0.18327\teval-mlogloss:0.24289\n",
      "[167]\ttrain-mlogloss:0.18281\teval-mlogloss:0.24308\n",
      "[168]\ttrain-mlogloss:0.18242\teval-mlogloss:0.24331\n",
      "[169]\ttrain-mlogloss:0.18212\teval-mlogloss:0.24346\n",
      "[170]\ttrain-mlogloss:0.18181\teval-mlogloss:0.24342\n",
      "[171]\ttrain-mlogloss:0.18149\teval-mlogloss:0.24336\n",
      "[172]\ttrain-mlogloss:0.18116\teval-mlogloss:0.24337\n",
      "[173]\ttrain-mlogloss:0.18069\teval-mlogloss:0.24348\n",
      "[174]\ttrain-mlogloss:0.18040\teval-mlogloss:0.24360\n",
      "[175]\ttrain-mlogloss:0.18007\teval-mlogloss:0.24364\n",
      "[176]\ttrain-mlogloss:0.17964\teval-mlogloss:0.24373\n",
      "[177]\ttrain-mlogloss:0.17940\teval-mlogloss:0.24377\n",
      "[178]\ttrain-mlogloss:0.17903\teval-mlogloss:0.24379\n",
      "[179]\ttrain-mlogloss:0.17872\teval-mlogloss:0.24390\n",
      "[180]\ttrain-mlogloss:0.17841\teval-mlogloss:0.24396\n",
      "[181]\ttrain-mlogloss:0.17800\teval-mlogloss:0.24420\n",
      "[182]\ttrain-mlogloss:0.17755\teval-mlogloss:0.24427\n",
      "[183]\ttrain-mlogloss:0.17727\teval-mlogloss:0.24421\n",
      "[184]\ttrain-mlogloss:0.17696\teval-mlogloss:0.24421\n",
      "[185]\ttrain-mlogloss:0.17662\teval-mlogloss:0.24439\n",
      "[186]\ttrain-mlogloss:0.17627\teval-mlogloss:0.24455\n",
      "[187]\ttrain-mlogloss:0.17600\teval-mlogloss:0.24440\n",
      "[188]\ttrain-mlogloss:0.17575\teval-mlogloss:0.24447\n",
      "[189]\ttrain-mlogloss:0.17545\teval-mlogloss:0.24430\n",
      "[190]\ttrain-mlogloss:0.17522\teval-mlogloss:0.24445\n",
      "[191]\ttrain-mlogloss:0.17488\teval-mlogloss:0.24441\n",
      "[192]\ttrain-mlogloss:0.17460\teval-mlogloss:0.24439\n",
      "[193]\ttrain-mlogloss:0.17428\teval-mlogloss:0.24446\n",
      "[194]\ttrain-mlogloss:0.17409\teval-mlogloss:0.24439\n",
      "[195]\ttrain-mlogloss:0.17388\teval-mlogloss:0.24442\n",
      "[196]\ttrain-mlogloss:0.17370\teval-mlogloss:0.24449\n",
      "[197]\ttrain-mlogloss:0.17338\teval-mlogloss:0.24463\n",
      "[198]\ttrain-mlogloss:0.17314\teval-mlogloss:0.24479\n",
      "[199]\ttrain-mlogloss:0.17288\teval-mlogloss:0.24479\n",
      "[200]\ttrain-mlogloss:0.17267\teval-mlogloss:0.24485\n",
      "[201]\ttrain-mlogloss:0.17241\teval-mlogloss:0.24485\n",
      "[202]\ttrain-mlogloss:0.17206\teval-mlogloss:0.24500\n",
      "[203]\ttrain-mlogloss:0.17186\teval-mlogloss:0.24502\n",
      "[204]\ttrain-mlogloss:0.17168\teval-mlogloss:0.24506\n",
      "[205]\ttrain-mlogloss:0.17148\teval-mlogloss:0.24510\n",
      "[206]\ttrain-mlogloss:0.17124\teval-mlogloss:0.24524\n",
      "[207]\ttrain-mlogloss:0.17087\teval-mlogloss:0.24516\n",
      "[208]\ttrain-mlogloss:0.17046\teval-mlogloss:0.24520\n",
      "[209]\ttrain-mlogloss:0.17020\teval-mlogloss:0.24520\n",
      "[210]\ttrain-mlogloss:0.16993\teval-mlogloss:0.24513\n",
      "[211]\ttrain-mlogloss:0.16977\teval-mlogloss:0.24512\n",
      "[212]\ttrain-mlogloss:0.16964\teval-mlogloss:0.24522\n",
      "[213]\ttrain-mlogloss:0.16929\teval-mlogloss:0.24520\n",
      "[214]\ttrain-mlogloss:0.16904\teval-mlogloss:0.24537\n",
      "[215]\ttrain-mlogloss:0.16889\teval-mlogloss:0.24530\n",
      "[216]\ttrain-mlogloss:0.16869\teval-mlogloss:0.24541\n",
      "[217]\ttrain-mlogloss:0.16857\teval-mlogloss:0.24537\n",
      "[218]\ttrain-mlogloss:0.16834\teval-mlogloss:0.24546\n",
      "[219]\ttrain-mlogloss:0.16816\teval-mlogloss:0.24556\n",
      "[220]\ttrain-mlogloss:0.16778\teval-mlogloss:0.24558\n",
      "[221]\ttrain-mlogloss:0.16752\teval-mlogloss:0.24543\n",
      "[222]\ttrain-mlogloss:0.16726\teval-mlogloss:0.24564\n",
      "[223]\ttrain-mlogloss:0.16695\teval-mlogloss:0.24567\n",
      "[224]\ttrain-mlogloss:0.16672\teval-mlogloss:0.24579\n",
      "[225]\ttrain-mlogloss:0.16638\teval-mlogloss:0.24584\n",
      "[226]\ttrain-mlogloss:0.16606\teval-mlogloss:0.24600\n",
      "xgb now score is: [2.5102825116552414]\n",
      "[0]\ttrain-mlogloss:0.67064\teval-mlogloss:0.67195\n",
      "[1]\ttrain-mlogloss:0.64923\teval-mlogloss:0.65159\n",
      "[2]\ttrain-mlogloss:0.62938\teval-mlogloss:0.63292\n",
      "[3]\ttrain-mlogloss:0.61044\teval-mlogloss:0.61526\n",
      "[4]\ttrain-mlogloss:0.59248\teval-mlogloss:0.59837\n",
      "[5]\ttrain-mlogloss:0.57534\teval-mlogloss:0.58226\n",
      "[6]\ttrain-mlogloss:0.55925\teval-mlogloss:0.56717\n",
      "[7]\ttrain-mlogloss:0.54382\teval-mlogloss:0.55285\n",
      "[8]\ttrain-mlogloss:0.52911\teval-mlogloss:0.53928\n",
      "[9]\ttrain-mlogloss:0.51521\teval-mlogloss:0.52632\n",
      "[10]\ttrain-mlogloss:0.50243\teval-mlogloss:0.51438\n",
      "[11]\ttrain-mlogloss:0.48973\teval-mlogloss:0.50247\n",
      "[12]\ttrain-mlogloss:0.47780\teval-mlogloss:0.49150\n",
      "[13]\ttrain-mlogloss:0.46645\teval-mlogloss:0.48098\n",
      "[14]\ttrain-mlogloss:0.45568\teval-mlogloss:0.47120\n",
      "[15]\ttrain-mlogloss:0.44550\teval-mlogloss:0.46182\n",
      "[16]\ttrain-mlogloss:0.43561\teval-mlogloss:0.45288\n",
      "[17]\ttrain-mlogloss:0.42618\teval-mlogloss:0.44442\n",
      "[18]\ttrain-mlogloss:0.41722\teval-mlogloss:0.43637\n",
      "[19]\ttrain-mlogloss:0.40858\teval-mlogloss:0.42855\n",
      "[20]\ttrain-mlogloss:0.40034\teval-mlogloss:0.42109\n",
      "[21]\ttrain-mlogloss:0.39247\teval-mlogloss:0.41408\n",
      "[22]\ttrain-mlogloss:0.38508\teval-mlogloss:0.40749\n",
      "[23]\ttrain-mlogloss:0.37789\teval-mlogloss:0.40122\n",
      "[24]\ttrain-mlogloss:0.37110\teval-mlogloss:0.39522\n",
      "[25]\ttrain-mlogloss:0.36463\teval-mlogloss:0.38954\n",
      "[26]\ttrain-mlogloss:0.35812\teval-mlogloss:0.38407\n",
      "[27]\ttrain-mlogloss:0.35208\teval-mlogloss:0.37881\n",
      "[28]\ttrain-mlogloss:0.34618\teval-mlogloss:0.37364\n",
      "[29]\ttrain-mlogloss:0.34057\teval-mlogloss:0.36882\n",
      "[30]\ttrain-mlogloss:0.33507\teval-mlogloss:0.36404\n",
      "[31]\ttrain-mlogloss:0.32988\teval-mlogloss:0.35980\n",
      "[32]\ttrain-mlogloss:0.32482\teval-mlogloss:0.35547\n",
      "[33]\ttrain-mlogloss:0.32008\teval-mlogloss:0.35152\n",
      "[34]\ttrain-mlogloss:0.31542\teval-mlogloss:0.34768\n",
      "[35]\ttrain-mlogloss:0.31102\teval-mlogloss:0.34425\n",
      "[36]\ttrain-mlogloss:0.30682\teval-mlogloss:0.34064\n",
      "[37]\ttrain-mlogloss:0.30279\teval-mlogloss:0.33736\n",
      "[38]\ttrain-mlogloss:0.29889\teval-mlogloss:0.33429\n",
      "[39]\ttrain-mlogloss:0.29514\teval-mlogloss:0.33115\n",
      "[40]\ttrain-mlogloss:0.29156\teval-mlogloss:0.32843\n",
      "[41]\ttrain-mlogloss:0.28809\teval-mlogloss:0.32595\n",
      "[42]\ttrain-mlogloss:0.28475\teval-mlogloss:0.32355\n",
      "[43]\ttrain-mlogloss:0.28163\teval-mlogloss:0.32117\n",
      "[44]\ttrain-mlogloss:0.27851\teval-mlogloss:0.31897\n",
      "[45]\ttrain-mlogloss:0.27558\teval-mlogloss:0.31679\n",
      "[46]\ttrain-mlogloss:0.27271\teval-mlogloss:0.31489\n",
      "[47]\ttrain-mlogloss:0.26985\teval-mlogloss:0.31288\n",
      "[48]\ttrain-mlogloss:0.26726\teval-mlogloss:0.31093\n",
      "[49]\ttrain-mlogloss:0.26493\teval-mlogloss:0.30916\n",
      "[50]\ttrain-mlogloss:0.26239\teval-mlogloss:0.30749\n",
      "[51]\ttrain-mlogloss:0.26007\teval-mlogloss:0.30595\n",
      "[52]\ttrain-mlogloss:0.25770\teval-mlogloss:0.30432\n",
      "[53]\ttrain-mlogloss:0.25535\teval-mlogloss:0.30282\n",
      "[54]\ttrain-mlogloss:0.25320\teval-mlogloss:0.30146\n",
      "[55]\ttrain-mlogloss:0.25127\teval-mlogloss:0.30018\n",
      "[56]\ttrain-mlogloss:0.24932\teval-mlogloss:0.29902\n",
      "[57]\ttrain-mlogloss:0.24762\teval-mlogloss:0.29780\n",
      "[58]\ttrain-mlogloss:0.24560\teval-mlogloss:0.29674\n",
      "[59]\ttrain-mlogloss:0.24379\teval-mlogloss:0.29574\n",
      "[60]\ttrain-mlogloss:0.24203\teval-mlogloss:0.29478\n",
      "[61]\ttrain-mlogloss:0.24044\teval-mlogloss:0.29377\n",
      "[62]\ttrain-mlogloss:0.23888\teval-mlogloss:0.29293\n",
      "[63]\ttrain-mlogloss:0.23745\teval-mlogloss:0.29228\n",
      "[64]\ttrain-mlogloss:0.23608\teval-mlogloss:0.29162\n",
      "[65]\ttrain-mlogloss:0.23462\teval-mlogloss:0.29089\n",
      "[66]\ttrain-mlogloss:0.23308\teval-mlogloss:0.28999\n",
      "[67]\ttrain-mlogloss:0.23178\teval-mlogloss:0.28952\n",
      "[68]\ttrain-mlogloss:0.23045\teval-mlogloss:0.28913\n",
      "[69]\ttrain-mlogloss:0.22919\teval-mlogloss:0.28852\n",
      "[70]\ttrain-mlogloss:0.22782\teval-mlogloss:0.28787\n",
      "[71]\ttrain-mlogloss:0.22661\teval-mlogloss:0.28725\n",
      "[72]\ttrain-mlogloss:0.22542\teval-mlogloss:0.28669\n",
      "[73]\ttrain-mlogloss:0.22433\teval-mlogloss:0.28637\n",
      "[74]\ttrain-mlogloss:0.22327\teval-mlogloss:0.28608\n",
      "[75]\ttrain-mlogloss:0.22215\teval-mlogloss:0.28584\n",
      "[76]\ttrain-mlogloss:0.22116\teval-mlogloss:0.28568\n",
      "[77]\ttrain-mlogloss:0.22020\teval-mlogloss:0.28548\n",
      "[78]\ttrain-mlogloss:0.21911\teval-mlogloss:0.28532\n",
      "[79]\ttrain-mlogloss:0.21800\teval-mlogloss:0.28510\n",
      "[80]\ttrain-mlogloss:0.21694\teval-mlogloss:0.28497\n",
      "[81]\ttrain-mlogloss:0.21580\teval-mlogloss:0.28483\n",
      "[82]\ttrain-mlogloss:0.21488\teval-mlogloss:0.28456\n",
      "[83]\ttrain-mlogloss:0.21396\teval-mlogloss:0.28447\n",
      "[84]\ttrain-mlogloss:0.21320\teval-mlogloss:0.28436\n",
      "[85]\ttrain-mlogloss:0.21225\teval-mlogloss:0.28438\n",
      "[86]\ttrain-mlogloss:0.21145\teval-mlogloss:0.28442\n",
      "[87]\ttrain-mlogloss:0.21063\teval-mlogloss:0.28445\n",
      "[88]\ttrain-mlogloss:0.20997\teval-mlogloss:0.28445\n",
      "[89]\ttrain-mlogloss:0.20904\teval-mlogloss:0.28459\n",
      "[90]\ttrain-mlogloss:0.20816\teval-mlogloss:0.28452\n",
      "[91]\ttrain-mlogloss:0.20751\teval-mlogloss:0.28447\n",
      "[92]\ttrain-mlogloss:0.20672\teval-mlogloss:0.28436\n",
      "[93]\ttrain-mlogloss:0.20570\teval-mlogloss:0.28423\n",
      "[94]\ttrain-mlogloss:0.20499\teval-mlogloss:0.28431\n",
      "[95]\ttrain-mlogloss:0.20411\teval-mlogloss:0.28428\n",
      "[96]\ttrain-mlogloss:0.20342\teval-mlogloss:0.28452\n",
      "[97]\ttrain-mlogloss:0.20262\teval-mlogloss:0.28467\n",
      "[98]\ttrain-mlogloss:0.20199\teval-mlogloss:0.28469\n",
      "[99]\ttrain-mlogloss:0.20112\teval-mlogloss:0.28479\n",
      "[100]\ttrain-mlogloss:0.20035\teval-mlogloss:0.28499\n",
      "[101]\ttrain-mlogloss:0.19961\teval-mlogloss:0.28497\n",
      "[102]\ttrain-mlogloss:0.19875\teval-mlogloss:0.28502\n",
      "[103]\ttrain-mlogloss:0.19827\teval-mlogloss:0.28531\n",
      "[104]\ttrain-mlogloss:0.19778\teval-mlogloss:0.28544\n",
      "[105]\ttrain-mlogloss:0.19725\teval-mlogloss:0.28565\n",
      "[106]\ttrain-mlogloss:0.19657\teval-mlogloss:0.28573\n",
      "[107]\ttrain-mlogloss:0.19592\teval-mlogloss:0.28604\n",
      "[108]\ttrain-mlogloss:0.19518\teval-mlogloss:0.28626\n",
      "[109]\ttrain-mlogloss:0.19456\teval-mlogloss:0.28648\n",
      "[110]\ttrain-mlogloss:0.19387\teval-mlogloss:0.28675\n",
      "[111]\ttrain-mlogloss:0.19335\teval-mlogloss:0.28698\n",
      "[112]\ttrain-mlogloss:0.19274\teval-mlogloss:0.28702\n",
      "[113]\ttrain-mlogloss:0.19220\teval-mlogloss:0.28710\n",
      "[114]\ttrain-mlogloss:0.19162\teval-mlogloss:0.28738\n",
      "[115]\ttrain-mlogloss:0.19106\teval-mlogloss:0.28748\n",
      "[116]\ttrain-mlogloss:0.19053\teval-mlogloss:0.28764\n",
      "[117]\ttrain-mlogloss:0.19008\teval-mlogloss:0.28772\n",
      "[118]\ttrain-mlogloss:0.18962\teval-mlogloss:0.28802\n",
      "[119]\ttrain-mlogloss:0.18901\teval-mlogloss:0.28809\n",
      "[120]\ttrain-mlogloss:0.18847\teval-mlogloss:0.28835\n",
      "[121]\ttrain-mlogloss:0.18795\teval-mlogloss:0.28859\n",
      "[122]\ttrain-mlogloss:0.18735\teval-mlogloss:0.28887\n",
      "[123]\ttrain-mlogloss:0.18667\teval-mlogloss:0.28934\n",
      "[124]\ttrain-mlogloss:0.18623\teval-mlogloss:0.28976\n",
      "[125]\ttrain-mlogloss:0.18570\teval-mlogloss:0.28988\n",
      "[126]\ttrain-mlogloss:0.18512\teval-mlogloss:0.29008\n",
      "[127]\ttrain-mlogloss:0.18462\teval-mlogloss:0.29051\n",
      "[128]\ttrain-mlogloss:0.18415\teval-mlogloss:0.29065\n",
      "[129]\ttrain-mlogloss:0.18358\teval-mlogloss:0.29071\n",
      "[130]\ttrain-mlogloss:0.18297\teval-mlogloss:0.29096\n",
      "[131]\ttrain-mlogloss:0.18248\teval-mlogloss:0.29115\n",
      "[132]\ttrain-mlogloss:0.18196\teval-mlogloss:0.29146\n",
      "[133]\ttrain-mlogloss:0.18147\teval-mlogloss:0.29155\n",
      "[134]\ttrain-mlogloss:0.18112\teval-mlogloss:0.29168\n",
      "[135]\ttrain-mlogloss:0.18065\teval-mlogloss:0.29199\n",
      "[136]\ttrain-mlogloss:0.18020\teval-mlogloss:0.29196\n",
      "[137]\ttrain-mlogloss:0.17970\teval-mlogloss:0.29241\n",
      "[138]\ttrain-mlogloss:0.17932\teval-mlogloss:0.29267\n",
      "[139]\ttrain-mlogloss:0.17904\teval-mlogloss:0.29279\n",
      "[140]\ttrain-mlogloss:0.17865\teval-mlogloss:0.29305\n",
      "[141]\ttrain-mlogloss:0.17817\teval-mlogloss:0.29316\n",
      "[142]\ttrain-mlogloss:0.17752\teval-mlogloss:0.29350\n",
      "[143]\ttrain-mlogloss:0.17708\teval-mlogloss:0.29384\n",
      "[144]\ttrain-mlogloss:0.17665\teval-mlogloss:0.29390\n",
      "[145]\ttrain-mlogloss:0.17622\teval-mlogloss:0.29414\n",
      "[146]\ttrain-mlogloss:0.17572\teval-mlogloss:0.29439\n",
      "[147]\ttrain-mlogloss:0.17527\teval-mlogloss:0.29448\n",
      "[148]\ttrain-mlogloss:0.17476\teval-mlogloss:0.29492\n",
      "[149]\ttrain-mlogloss:0.17440\teval-mlogloss:0.29512\n",
      "[150]\ttrain-mlogloss:0.17392\teval-mlogloss:0.29548\n",
      "[151]\ttrain-mlogloss:0.17351\teval-mlogloss:0.29546\n",
      "[152]\ttrain-mlogloss:0.17322\teval-mlogloss:0.29587\n",
      "[153]\ttrain-mlogloss:0.17280\teval-mlogloss:0.29625\n",
      "[154]\ttrain-mlogloss:0.17247\teval-mlogloss:0.29652\n",
      "[155]\ttrain-mlogloss:0.17204\teval-mlogloss:0.29681\n",
      "[156]\ttrain-mlogloss:0.17168\teval-mlogloss:0.29689\n",
      "[157]\ttrain-mlogloss:0.17111\teval-mlogloss:0.29698\n",
      "[158]\ttrain-mlogloss:0.17072\teval-mlogloss:0.29718\n",
      "[159]\ttrain-mlogloss:0.17042\teval-mlogloss:0.29731\n",
      "[160]\ttrain-mlogloss:0.17001\teval-mlogloss:0.29762\n",
      "[161]\ttrain-mlogloss:0.16964\teval-mlogloss:0.29787\n",
      "[162]\ttrain-mlogloss:0.16925\teval-mlogloss:0.29819\n",
      "[163]\ttrain-mlogloss:0.16893\teval-mlogloss:0.29848\n",
      "[164]\ttrain-mlogloss:0.16864\teval-mlogloss:0.29843\n",
      "[165]\ttrain-mlogloss:0.16829\teval-mlogloss:0.29869\n",
      "[166]\ttrain-mlogloss:0.16790\teval-mlogloss:0.29904\n",
      "[167]\ttrain-mlogloss:0.16750\teval-mlogloss:0.29919\n",
      "[168]\ttrain-mlogloss:0.16710\teval-mlogloss:0.29923\n",
      "[169]\ttrain-mlogloss:0.16677\teval-mlogloss:0.29969\n",
      "[170]\ttrain-mlogloss:0.16647\teval-mlogloss:0.29992\n",
      "[171]\ttrain-mlogloss:0.16626\teval-mlogloss:0.30005\n",
      "[172]\ttrain-mlogloss:0.16603\teval-mlogloss:0.30007\n",
      "[173]\ttrain-mlogloss:0.16575\teval-mlogloss:0.30025\n",
      "[174]\ttrain-mlogloss:0.16542\teval-mlogloss:0.30050\n",
      "[175]\ttrain-mlogloss:0.16514\teval-mlogloss:0.30062\n",
      "[176]\ttrain-mlogloss:0.16478\teval-mlogloss:0.30104\n",
      "[177]\ttrain-mlogloss:0.16433\teval-mlogloss:0.30114\n",
      "[178]\ttrain-mlogloss:0.16394\teval-mlogloss:0.30137\n",
      "[179]\ttrain-mlogloss:0.16361\teval-mlogloss:0.30151\n",
      "[180]\ttrain-mlogloss:0.16340\teval-mlogloss:0.30151\n",
      "[181]\ttrain-mlogloss:0.16308\teval-mlogloss:0.30140\n",
      "[182]\ttrain-mlogloss:0.16274\teval-mlogloss:0.30176\n",
      "[183]\ttrain-mlogloss:0.16248\teval-mlogloss:0.30177\n",
      "[184]\ttrain-mlogloss:0.16206\teval-mlogloss:0.30187\n",
      "[185]\ttrain-mlogloss:0.16186\teval-mlogloss:0.30208\n",
      "[186]\ttrain-mlogloss:0.16151\teval-mlogloss:0.30233\n",
      "[187]\ttrain-mlogloss:0.16120\teval-mlogloss:0.30245\n",
      "[188]\ttrain-mlogloss:0.16092\teval-mlogloss:0.30293\n",
      "[189]\ttrain-mlogloss:0.16073\teval-mlogloss:0.30304\n",
      "[190]\ttrain-mlogloss:0.16045\teval-mlogloss:0.30344\n",
      "[191]\ttrain-mlogloss:0.16002\teval-mlogloss:0.30354\n",
      "[192]\ttrain-mlogloss:0.15973\teval-mlogloss:0.30366\n",
      "[193]\ttrain-mlogloss:0.15957\teval-mlogloss:0.30350\n",
      "xgb now score is: [2.5102825116552414, 2.2800363773293792]\n",
      "[0]\ttrain-mlogloss:0.67063\teval-mlogloss:0.67126\n",
      "[1]\ttrain-mlogloss:0.64952\teval-mlogloss:0.65065\n",
      "[2]\ttrain-mlogloss:0.62972\teval-mlogloss:0.63169\n",
      "[3]\ttrain-mlogloss:0.61110\teval-mlogloss:0.61376\n",
      "[4]\ttrain-mlogloss:0.59335\teval-mlogloss:0.59668\n",
      "[5]\ttrain-mlogloss:0.57634\teval-mlogloss:0.58044\n",
      "[6]\ttrain-mlogloss:0.56051\teval-mlogloss:0.56522\n",
      "[7]\ttrain-mlogloss:0.54510\teval-mlogloss:0.55031\n",
      "[8]\ttrain-mlogloss:0.53037\teval-mlogloss:0.53631\n",
      "[9]\ttrain-mlogloss:0.51662\teval-mlogloss:0.52311\n",
      "[10]\ttrain-mlogloss:0.50358\teval-mlogloss:0.51071\n",
      "[11]\ttrain-mlogloss:0.49111\teval-mlogloss:0.49880\n",
      "[12]\ttrain-mlogloss:0.47946\teval-mlogloss:0.48759\n",
      "[13]\ttrain-mlogloss:0.46808\teval-mlogloss:0.47689\n",
      "[14]\ttrain-mlogloss:0.45734\teval-mlogloss:0.46653\n",
      "[15]\ttrain-mlogloss:0.44693\teval-mlogloss:0.45673\n",
      "[16]\ttrain-mlogloss:0.43701\teval-mlogloss:0.44747\n",
      "[17]\ttrain-mlogloss:0.42755\teval-mlogloss:0.43862\n",
      "[18]\ttrain-mlogloss:0.41867\teval-mlogloss:0.43028\n",
      "[19]\ttrain-mlogloss:0.41019\teval-mlogloss:0.42230\n",
      "[20]\ttrain-mlogloss:0.40201\teval-mlogloss:0.41465\n",
      "[21]\ttrain-mlogloss:0.39426\teval-mlogloss:0.40757\n",
      "[22]\ttrain-mlogloss:0.38672\teval-mlogloss:0.40061\n",
      "[23]\ttrain-mlogloss:0.37970\teval-mlogloss:0.39409\n",
      "[24]\ttrain-mlogloss:0.37291\teval-mlogloss:0.38776\n",
      "[25]\ttrain-mlogloss:0.36632\teval-mlogloss:0.38166\n",
      "[26]\ttrain-mlogloss:0.36005\teval-mlogloss:0.37575\n",
      "[27]\ttrain-mlogloss:0.35400\teval-mlogloss:0.37024\n",
      "[28]\ttrain-mlogloss:0.34811\teval-mlogloss:0.36490\n",
      "[29]\ttrain-mlogloss:0.34265\teval-mlogloss:0.35991\n",
      "[30]\ttrain-mlogloss:0.33717\teval-mlogloss:0.35502\n",
      "[31]\ttrain-mlogloss:0.33213\teval-mlogloss:0.35051\n",
      "[32]\ttrain-mlogloss:0.32723\teval-mlogloss:0.34616\n",
      "[33]\ttrain-mlogloss:0.32263\teval-mlogloss:0.34219\n",
      "[34]\ttrain-mlogloss:0.31800\teval-mlogloss:0.33803\n",
      "[35]\ttrain-mlogloss:0.31375\teval-mlogloss:0.33429\n",
      "[36]\ttrain-mlogloss:0.30955\teval-mlogloss:0.33057\n",
      "[37]\ttrain-mlogloss:0.30539\teval-mlogloss:0.32713\n",
      "[38]\ttrain-mlogloss:0.30138\teval-mlogloss:0.32383\n",
      "[39]\ttrain-mlogloss:0.29774\teval-mlogloss:0.32067\n",
      "[40]\ttrain-mlogloss:0.29424\teval-mlogloss:0.31766\n",
      "[41]\ttrain-mlogloss:0.29068\teval-mlogloss:0.31489\n",
      "[42]\ttrain-mlogloss:0.28720\teval-mlogloss:0.31207\n",
      "[43]\ttrain-mlogloss:0.28406\teval-mlogloss:0.30948\n",
      "[44]\ttrain-mlogloss:0.28108\teval-mlogloss:0.30708\n",
      "[45]\ttrain-mlogloss:0.27818\teval-mlogloss:0.30476\n",
      "[46]\ttrain-mlogloss:0.27545\teval-mlogloss:0.30258\n",
      "[47]\ttrain-mlogloss:0.27268\teval-mlogloss:0.30040\n",
      "[48]\ttrain-mlogloss:0.26986\teval-mlogloss:0.29830\n",
      "[49]\ttrain-mlogloss:0.26715\teval-mlogloss:0.29637\n",
      "[50]\ttrain-mlogloss:0.26458\teval-mlogloss:0.29441\n",
      "[51]\ttrain-mlogloss:0.26230\teval-mlogloss:0.29268\n",
      "[52]\ttrain-mlogloss:0.26005\teval-mlogloss:0.29090\n",
      "[53]\ttrain-mlogloss:0.25786\teval-mlogloss:0.28934\n",
      "[54]\ttrain-mlogloss:0.25561\teval-mlogloss:0.28775\n",
      "[55]\ttrain-mlogloss:0.25352\teval-mlogloss:0.28628\n",
      "[56]\ttrain-mlogloss:0.25159\teval-mlogloss:0.28491\n",
      "[57]\ttrain-mlogloss:0.24967\teval-mlogloss:0.28362\n",
      "[58]\ttrain-mlogloss:0.24789\teval-mlogloss:0.28243\n",
      "[59]\ttrain-mlogloss:0.24585\teval-mlogloss:0.28113\n",
      "[60]\ttrain-mlogloss:0.24403\teval-mlogloss:0.28009\n",
      "[61]\ttrain-mlogloss:0.24241\teval-mlogloss:0.27908\n",
      "[62]\ttrain-mlogloss:0.24082\teval-mlogloss:0.27806\n",
      "[63]\ttrain-mlogloss:0.23935\teval-mlogloss:0.27709\n",
      "[64]\ttrain-mlogloss:0.23785\teval-mlogloss:0.27607\n",
      "[65]\ttrain-mlogloss:0.23642\teval-mlogloss:0.27516\n",
      "[66]\ttrain-mlogloss:0.23502\teval-mlogloss:0.27428\n",
      "[67]\ttrain-mlogloss:0.23353\teval-mlogloss:0.27354\n",
      "[68]\ttrain-mlogloss:0.23227\teval-mlogloss:0.27290\n",
      "[69]\ttrain-mlogloss:0.23101\teval-mlogloss:0.27224\n",
      "[70]\ttrain-mlogloss:0.22975\teval-mlogloss:0.27156\n",
      "[71]\ttrain-mlogloss:0.22850\teval-mlogloss:0.27097\n",
      "[72]\ttrain-mlogloss:0.22736\teval-mlogloss:0.27037\n",
      "[73]\ttrain-mlogloss:0.22626\teval-mlogloss:0.26985\n",
      "[74]\ttrain-mlogloss:0.22512\teval-mlogloss:0.26919\n",
      "[75]\ttrain-mlogloss:0.22406\teval-mlogloss:0.26881\n",
      "[76]\ttrain-mlogloss:0.22295\teval-mlogloss:0.26831\n",
      "[77]\ttrain-mlogloss:0.22200\teval-mlogloss:0.26784\n",
      "[78]\ttrain-mlogloss:0.22103\teval-mlogloss:0.26746\n",
      "[79]\ttrain-mlogloss:0.22022\teval-mlogloss:0.26696\n",
      "[80]\ttrain-mlogloss:0.21936\teval-mlogloss:0.26662\n",
      "[81]\ttrain-mlogloss:0.21860\teval-mlogloss:0.26645\n",
      "[82]\ttrain-mlogloss:0.21770\teval-mlogloss:0.26603\n",
      "[83]\ttrain-mlogloss:0.21673\teval-mlogloss:0.26582\n",
      "[84]\ttrain-mlogloss:0.21593\teval-mlogloss:0.26551\n",
      "[85]\ttrain-mlogloss:0.21519\teval-mlogloss:0.26535\n",
      "[86]\ttrain-mlogloss:0.21414\teval-mlogloss:0.26518\n",
      "[87]\ttrain-mlogloss:0.21351\teval-mlogloss:0.26495\n",
      "[88]\ttrain-mlogloss:0.21260\teval-mlogloss:0.26480\n",
      "[89]\ttrain-mlogloss:0.21191\teval-mlogloss:0.26475\n",
      "[90]\ttrain-mlogloss:0.21116\teval-mlogloss:0.26458\n",
      "[91]\ttrain-mlogloss:0.21054\teval-mlogloss:0.26436\n",
      "[92]\ttrain-mlogloss:0.20965\teval-mlogloss:0.26437\n",
      "[93]\ttrain-mlogloss:0.20894\teval-mlogloss:0.26438\n",
      "[94]\ttrain-mlogloss:0.20820\teval-mlogloss:0.26415\n",
      "[95]\ttrain-mlogloss:0.20738\teval-mlogloss:0.26404\n",
      "[96]\ttrain-mlogloss:0.20665\teval-mlogloss:0.26407\n",
      "[97]\ttrain-mlogloss:0.20607\teval-mlogloss:0.26379\n",
      "[98]\ttrain-mlogloss:0.20546\teval-mlogloss:0.26383\n",
      "[99]\ttrain-mlogloss:0.20490\teval-mlogloss:0.26360\n",
      "[100]\ttrain-mlogloss:0.20429\teval-mlogloss:0.26344\n",
      "[101]\ttrain-mlogloss:0.20361\teval-mlogloss:0.26358\n",
      "[102]\ttrain-mlogloss:0.20294\teval-mlogloss:0.26368\n",
      "[103]\ttrain-mlogloss:0.20231\teval-mlogloss:0.26375\n",
      "[104]\ttrain-mlogloss:0.20183\teval-mlogloss:0.26392\n",
      "[105]\ttrain-mlogloss:0.20130\teval-mlogloss:0.26384\n",
      "[106]\ttrain-mlogloss:0.20079\teval-mlogloss:0.26380\n",
      "[107]\ttrain-mlogloss:0.20032\teval-mlogloss:0.26378\n",
      "[108]\ttrain-mlogloss:0.19963\teval-mlogloss:0.26384\n",
      "[109]\ttrain-mlogloss:0.19910\teval-mlogloss:0.26388\n",
      "[110]\ttrain-mlogloss:0.19856\teval-mlogloss:0.26396\n",
      "[111]\ttrain-mlogloss:0.19800\teval-mlogloss:0.26395\n",
      "[112]\ttrain-mlogloss:0.19754\teval-mlogloss:0.26409\n",
      "[113]\ttrain-mlogloss:0.19711\teval-mlogloss:0.26401\n",
      "[114]\ttrain-mlogloss:0.19659\teval-mlogloss:0.26425\n",
      "[115]\ttrain-mlogloss:0.19595\teval-mlogloss:0.26433\n",
      "[116]\ttrain-mlogloss:0.19545\teval-mlogloss:0.26435\n",
      "[117]\ttrain-mlogloss:0.19490\teval-mlogloss:0.26454\n",
      "[118]\ttrain-mlogloss:0.19442\teval-mlogloss:0.26483\n",
      "[119]\ttrain-mlogloss:0.19393\teval-mlogloss:0.26501\n",
      "[120]\ttrain-mlogloss:0.19359\teval-mlogloss:0.26512\n",
      "[121]\ttrain-mlogloss:0.19299\teval-mlogloss:0.26543\n",
      "[122]\ttrain-mlogloss:0.19244\teval-mlogloss:0.26535\n",
      "[123]\ttrain-mlogloss:0.19190\teval-mlogloss:0.26545\n",
      "[124]\ttrain-mlogloss:0.19154\teval-mlogloss:0.26547\n",
      "[125]\ttrain-mlogloss:0.19106\teval-mlogloss:0.26558\n",
      "[126]\ttrain-mlogloss:0.19066\teval-mlogloss:0.26572\n",
      "[127]\ttrain-mlogloss:0.19006\teval-mlogloss:0.26565\n",
      "[128]\ttrain-mlogloss:0.18957\teval-mlogloss:0.26598\n",
      "[129]\ttrain-mlogloss:0.18917\teval-mlogloss:0.26613\n",
      "[130]\ttrain-mlogloss:0.18868\teval-mlogloss:0.26625\n",
      "[131]\ttrain-mlogloss:0.18823\teval-mlogloss:0.26647\n",
      "[132]\ttrain-mlogloss:0.18783\teval-mlogloss:0.26685\n",
      "[133]\ttrain-mlogloss:0.18732\teval-mlogloss:0.26692\n",
      "[134]\ttrain-mlogloss:0.18676\teval-mlogloss:0.26677\n",
      "[135]\ttrain-mlogloss:0.18621\teval-mlogloss:0.26706\n",
      "[136]\ttrain-mlogloss:0.18593\teval-mlogloss:0.26718\n",
      "[137]\ttrain-mlogloss:0.18564\teval-mlogloss:0.26723\n",
      "[138]\ttrain-mlogloss:0.18515\teval-mlogloss:0.26733\n",
      "[139]\ttrain-mlogloss:0.18475\teval-mlogloss:0.26739\n",
      "[140]\ttrain-mlogloss:0.18426\teval-mlogloss:0.26748\n",
      "[141]\ttrain-mlogloss:0.18402\teval-mlogloss:0.26742\n",
      "[142]\ttrain-mlogloss:0.18356\teval-mlogloss:0.26753\n",
      "[143]\ttrain-mlogloss:0.18324\teval-mlogloss:0.26760\n",
      "[144]\ttrain-mlogloss:0.18279\teval-mlogloss:0.26772\n",
      "[145]\ttrain-mlogloss:0.18233\teval-mlogloss:0.26770\n",
      "[146]\ttrain-mlogloss:0.18195\teval-mlogloss:0.26804\n",
      "[147]\ttrain-mlogloss:0.18158\teval-mlogloss:0.26823\n",
      "[148]\ttrain-mlogloss:0.18130\teval-mlogloss:0.26836\n",
      "[149]\ttrain-mlogloss:0.18087\teval-mlogloss:0.26850\n",
      "[150]\ttrain-mlogloss:0.18042\teval-mlogloss:0.26869\n",
      "[151]\ttrain-mlogloss:0.17996\teval-mlogloss:0.26872\n",
      "[152]\ttrain-mlogloss:0.17960\teval-mlogloss:0.26901\n",
      "[153]\ttrain-mlogloss:0.17913\teval-mlogloss:0.26922\n",
      "[154]\ttrain-mlogloss:0.17888\teval-mlogloss:0.26924\n",
      "[155]\ttrain-mlogloss:0.17851\teval-mlogloss:0.26939\n",
      "[156]\ttrain-mlogloss:0.17818\teval-mlogloss:0.26954\n",
      "[157]\ttrain-mlogloss:0.17783\teval-mlogloss:0.26974\n",
      "[158]\ttrain-mlogloss:0.17738\teval-mlogloss:0.26996\n",
      "[159]\ttrain-mlogloss:0.17698\teval-mlogloss:0.27023\n",
      "[160]\ttrain-mlogloss:0.17666\teval-mlogloss:0.27024\n",
      "[161]\ttrain-mlogloss:0.17634\teval-mlogloss:0.27027\n",
      "[162]\ttrain-mlogloss:0.17592\teval-mlogloss:0.27021\n",
      "[163]\ttrain-mlogloss:0.17561\teval-mlogloss:0.27036\n",
      "[164]\ttrain-mlogloss:0.17524\teval-mlogloss:0.27036\n",
      "[165]\ttrain-mlogloss:0.17485\teval-mlogloss:0.27063\n",
      "[166]\ttrain-mlogloss:0.17446\teval-mlogloss:0.27088\n",
      "[167]\ttrain-mlogloss:0.17415\teval-mlogloss:0.27082\n",
      "[168]\ttrain-mlogloss:0.17400\teval-mlogloss:0.27103\n",
      "[169]\ttrain-mlogloss:0.17374\teval-mlogloss:0.27122\n",
      "[170]\ttrain-mlogloss:0.17343\teval-mlogloss:0.27134\n",
      "[171]\ttrain-mlogloss:0.17297\teval-mlogloss:0.27145\n",
      "[172]\ttrain-mlogloss:0.17261\teval-mlogloss:0.27177\n",
      "[173]\ttrain-mlogloss:0.17226\teval-mlogloss:0.27190\n",
      "[174]\ttrain-mlogloss:0.17185\teval-mlogloss:0.27216\n",
      "[175]\ttrain-mlogloss:0.17154\teval-mlogloss:0.27217\n",
      "[176]\ttrain-mlogloss:0.17122\teval-mlogloss:0.27225\n",
      "[177]\ttrain-mlogloss:0.17085\teval-mlogloss:0.27221\n",
      "[178]\ttrain-mlogloss:0.17053\teval-mlogloss:0.27219\n",
      "[179]\ttrain-mlogloss:0.17017\teval-mlogloss:0.27206\n",
      "[180]\ttrain-mlogloss:0.16988\teval-mlogloss:0.27209\n",
      "[181]\ttrain-mlogloss:0.16966\teval-mlogloss:0.27236\n",
      "[182]\ttrain-mlogloss:0.16931\teval-mlogloss:0.27235\n",
      "[183]\ttrain-mlogloss:0.16907\teval-mlogloss:0.27245\n",
      "[184]\ttrain-mlogloss:0.16881\teval-mlogloss:0.27275\n",
      "[185]\ttrain-mlogloss:0.16844\teval-mlogloss:0.27283\n",
      "[186]\ttrain-mlogloss:0.16820\teval-mlogloss:0.27302\n",
      "[187]\ttrain-mlogloss:0.16793\teval-mlogloss:0.27313\n",
      "[188]\ttrain-mlogloss:0.16759\teval-mlogloss:0.27319\n",
      "[189]\ttrain-mlogloss:0.16724\teval-mlogloss:0.27346\n",
      "[190]\ttrain-mlogloss:0.16691\teval-mlogloss:0.27340\n",
      "[191]\ttrain-mlogloss:0.16672\teval-mlogloss:0.27345\n",
      "[192]\ttrain-mlogloss:0.16648\teval-mlogloss:0.27366\n",
      "[193]\ttrain-mlogloss:0.16618\teval-mlogloss:0.27387\n",
      "[194]\ttrain-mlogloss:0.16595\teval-mlogloss:0.27392\n",
      "[195]\ttrain-mlogloss:0.16568\teval-mlogloss:0.27398\n",
      "[196]\ttrain-mlogloss:0.16535\teval-mlogloss:0.27408\n",
      "[197]\ttrain-mlogloss:0.16500\teval-mlogloss:0.27408\n",
      "[198]\ttrain-mlogloss:0.16474\teval-mlogloss:0.27411\n",
      "[199]\ttrain-mlogloss:0.16451\teval-mlogloss:0.27423\n",
      "[200]\ttrain-mlogloss:0.16427\teval-mlogloss:0.27431\n",
      "xgb now score is: [2.5102825116552414, 2.2800363773293792, 2.366547712795436]\n",
      "[0]\ttrain-mlogloss:0.67117\teval-mlogloss:0.67061\n",
      "[1]\ttrain-mlogloss:0.65042\teval-mlogloss:0.64927\n",
      "[2]\ttrain-mlogloss:0.63088\teval-mlogloss:0.62935\n",
      "[3]\ttrain-mlogloss:0.61255\teval-mlogloss:0.61048\n",
      "[4]\ttrain-mlogloss:0.59525\teval-mlogloss:0.59262\n",
      "[5]\ttrain-mlogloss:0.57878\teval-mlogloss:0.57572\n",
      "[6]\ttrain-mlogloss:0.56335\teval-mlogloss:0.55983\n",
      "[7]\ttrain-mlogloss:0.54846\teval-mlogloss:0.54444\n",
      "[8]\ttrain-mlogloss:0.53432\teval-mlogloss:0.52970\n",
      "[9]\ttrain-mlogloss:0.52092\teval-mlogloss:0.51574\n",
      "[10]\ttrain-mlogloss:0.50828\teval-mlogloss:0.50284\n",
      "[11]\ttrain-mlogloss:0.49620\teval-mlogloss:0.49038\n",
      "[12]\ttrain-mlogloss:0.48457\teval-mlogloss:0.47835\n",
      "[13]\ttrain-mlogloss:0.47368\teval-mlogloss:0.46703\n",
      "[14]\ttrain-mlogloss:0.46317\teval-mlogloss:0.45632\n",
      "[15]\ttrain-mlogloss:0.45329\teval-mlogloss:0.44600\n",
      "[16]\ttrain-mlogloss:0.44378\teval-mlogloss:0.43619\n",
      "[17]\ttrain-mlogloss:0.43472\teval-mlogloss:0.42671\n",
      "[18]\ttrain-mlogloss:0.42602\teval-mlogloss:0.41770\n",
      "[19]\ttrain-mlogloss:0.41776\teval-mlogloss:0.40919\n",
      "[20]\ttrain-mlogloss:0.40987\teval-mlogloss:0.40098\n",
      "[21]\ttrain-mlogloss:0.40220\teval-mlogloss:0.39297\n",
      "[22]\ttrain-mlogloss:0.39498\teval-mlogloss:0.38554\n",
      "[23]\ttrain-mlogloss:0.38787\teval-mlogloss:0.37814\n",
      "[24]\ttrain-mlogloss:0.38130\teval-mlogloss:0.37122\n",
      "[25]\ttrain-mlogloss:0.37483\teval-mlogloss:0.36465\n",
      "[26]\ttrain-mlogloss:0.36879\teval-mlogloss:0.35841\n",
      "[27]\ttrain-mlogloss:0.36299\teval-mlogloss:0.35250\n",
      "[28]\ttrain-mlogloss:0.35739\teval-mlogloss:0.34673\n",
      "[29]\ttrain-mlogloss:0.35215\teval-mlogloss:0.34117\n",
      "[30]\ttrain-mlogloss:0.34710\teval-mlogloss:0.33587\n",
      "[31]\ttrain-mlogloss:0.34221\teval-mlogloss:0.33067\n",
      "[32]\ttrain-mlogloss:0.33749\teval-mlogloss:0.32575\n",
      "[33]\ttrain-mlogloss:0.33293\teval-mlogloss:0.32106\n",
      "[34]\ttrain-mlogloss:0.32858\teval-mlogloss:0.31658\n",
      "[35]\ttrain-mlogloss:0.32433\teval-mlogloss:0.31231\n",
      "[36]\ttrain-mlogloss:0.32025\teval-mlogloss:0.30816\n",
      "[37]\ttrain-mlogloss:0.31651\teval-mlogloss:0.30435\n",
      "[38]\ttrain-mlogloss:0.31281\teval-mlogloss:0.30063\n",
      "[39]\ttrain-mlogloss:0.30921\teval-mlogloss:0.29700\n",
      "[40]\ttrain-mlogloss:0.30576\teval-mlogloss:0.29344\n",
      "[41]\ttrain-mlogloss:0.30244\teval-mlogloss:0.28998\n",
      "[42]\ttrain-mlogloss:0.29933\teval-mlogloss:0.28659\n",
      "[43]\ttrain-mlogloss:0.29633\teval-mlogloss:0.28352\n",
      "[44]\ttrain-mlogloss:0.29345\teval-mlogloss:0.28054\n",
      "[45]\ttrain-mlogloss:0.29058\teval-mlogloss:0.27743\n",
      "[46]\ttrain-mlogloss:0.28787\teval-mlogloss:0.27462\n",
      "[47]\ttrain-mlogloss:0.28528\teval-mlogloss:0.27205\n",
      "[48]\ttrain-mlogloss:0.28285\teval-mlogloss:0.26947\n",
      "[49]\ttrain-mlogloss:0.28066\teval-mlogloss:0.26719\n",
      "[50]\ttrain-mlogloss:0.27835\teval-mlogloss:0.26477\n",
      "[51]\ttrain-mlogloss:0.27591\teval-mlogloss:0.26239\n",
      "[52]\ttrain-mlogloss:0.27374\teval-mlogloss:0.26001\n",
      "[53]\ttrain-mlogloss:0.27146\teval-mlogloss:0.25775\n",
      "[54]\ttrain-mlogloss:0.26956\teval-mlogloss:0.25587\n",
      "[55]\ttrain-mlogloss:0.26764\teval-mlogloss:0.25388\n",
      "[56]\ttrain-mlogloss:0.26586\teval-mlogloss:0.25211\n",
      "[57]\ttrain-mlogloss:0.26401\teval-mlogloss:0.25037\n",
      "[58]\ttrain-mlogloss:0.26212\teval-mlogloss:0.24847\n",
      "[59]\ttrain-mlogloss:0.26031\teval-mlogloss:0.24676\n",
      "[60]\ttrain-mlogloss:0.25872\teval-mlogloss:0.24517\n",
      "[61]\ttrain-mlogloss:0.25721\teval-mlogloss:0.24369\n",
      "[62]\ttrain-mlogloss:0.25567\teval-mlogloss:0.24229\n",
      "[63]\ttrain-mlogloss:0.25412\teval-mlogloss:0.24088\n",
      "[64]\ttrain-mlogloss:0.25246\teval-mlogloss:0.23946\n",
      "[65]\ttrain-mlogloss:0.25089\teval-mlogloss:0.23806\n",
      "[66]\ttrain-mlogloss:0.24964\teval-mlogloss:0.23689\n",
      "[67]\ttrain-mlogloss:0.24828\teval-mlogloss:0.23578\n",
      "[68]\ttrain-mlogloss:0.24712\teval-mlogloss:0.23465\n",
      "[69]\ttrain-mlogloss:0.24571\teval-mlogloss:0.23361\n",
      "[70]\ttrain-mlogloss:0.24436\teval-mlogloss:0.23237\n",
      "[71]\ttrain-mlogloss:0.24302\teval-mlogloss:0.23118\n",
      "[72]\ttrain-mlogloss:0.24196\teval-mlogloss:0.23023\n",
      "[73]\ttrain-mlogloss:0.24098\teval-mlogloss:0.22938\n",
      "[74]\ttrain-mlogloss:0.23997\teval-mlogloss:0.22857\n",
      "[75]\ttrain-mlogloss:0.23893\teval-mlogloss:0.22774\n",
      "[76]\ttrain-mlogloss:0.23805\teval-mlogloss:0.22707\n",
      "[77]\ttrain-mlogloss:0.23711\teval-mlogloss:0.22638\n",
      "[78]\ttrain-mlogloss:0.23630\teval-mlogloss:0.22569\n",
      "[79]\ttrain-mlogloss:0.23543\teval-mlogloss:0.22480\n",
      "[80]\ttrain-mlogloss:0.23463\teval-mlogloss:0.22402\n",
      "[81]\ttrain-mlogloss:0.23375\teval-mlogloss:0.22349\n",
      "[82]\ttrain-mlogloss:0.23289\teval-mlogloss:0.22274\n",
      "[83]\ttrain-mlogloss:0.23203\teval-mlogloss:0.22214\n",
      "[84]\ttrain-mlogloss:0.23132\teval-mlogloss:0.22147\n",
      "[85]\ttrain-mlogloss:0.23043\teval-mlogloss:0.22090\n",
      "[86]\ttrain-mlogloss:0.22979\teval-mlogloss:0.22043\n",
      "[87]\ttrain-mlogloss:0.22903\teval-mlogloss:0.21996\n",
      "[88]\ttrain-mlogloss:0.22837\teval-mlogloss:0.21947\n",
      "[89]\ttrain-mlogloss:0.22766\teval-mlogloss:0.21905\n",
      "[90]\ttrain-mlogloss:0.22686\teval-mlogloss:0.21852\n",
      "[91]\ttrain-mlogloss:0.22629\teval-mlogloss:0.21811\n",
      "[92]\ttrain-mlogloss:0.22554\teval-mlogloss:0.21787\n",
      "[93]\ttrain-mlogloss:0.22473\teval-mlogloss:0.21733\n",
      "[94]\ttrain-mlogloss:0.22407\teval-mlogloss:0.21699\n",
      "[95]\ttrain-mlogloss:0.22352\teval-mlogloss:0.21646\n",
      "[96]\ttrain-mlogloss:0.22286\teval-mlogloss:0.21599\n",
      "[97]\ttrain-mlogloss:0.22225\teval-mlogloss:0.21565\n",
      "[98]\ttrain-mlogloss:0.22168\teval-mlogloss:0.21521\n",
      "[99]\ttrain-mlogloss:0.22092\teval-mlogloss:0.21475\n",
      "[100]\ttrain-mlogloss:0.22048\teval-mlogloss:0.21444\n",
      "[101]\ttrain-mlogloss:0.21992\teval-mlogloss:0.21399\n",
      "[102]\ttrain-mlogloss:0.21927\teval-mlogloss:0.21367\n",
      "[103]\ttrain-mlogloss:0.21846\teval-mlogloss:0.21346\n",
      "[104]\ttrain-mlogloss:0.21792\teval-mlogloss:0.21333\n",
      "[105]\ttrain-mlogloss:0.21751\teval-mlogloss:0.21306\n",
      "[106]\ttrain-mlogloss:0.21717\teval-mlogloss:0.21283\n",
      "[107]\ttrain-mlogloss:0.21668\teval-mlogloss:0.21244\n",
      "[108]\ttrain-mlogloss:0.21605\teval-mlogloss:0.21234\n",
      "[109]\ttrain-mlogloss:0.21553\teval-mlogloss:0.21207\n",
      "[110]\ttrain-mlogloss:0.21508\teval-mlogloss:0.21203\n",
      "[111]\ttrain-mlogloss:0.21453\teval-mlogloss:0.21199\n",
      "[112]\ttrain-mlogloss:0.21398\teval-mlogloss:0.21171\n",
      "[113]\ttrain-mlogloss:0.21353\teval-mlogloss:0.21160\n",
      "[114]\ttrain-mlogloss:0.21306\teval-mlogloss:0.21146\n",
      "[115]\ttrain-mlogloss:0.21244\teval-mlogloss:0.21110\n",
      "[116]\ttrain-mlogloss:0.21192\teval-mlogloss:0.21102\n",
      "[117]\ttrain-mlogloss:0.21130\teval-mlogloss:0.21071\n",
      "[118]\ttrain-mlogloss:0.21082\teval-mlogloss:0.21055\n",
      "[119]\ttrain-mlogloss:0.21030\teval-mlogloss:0.21044\n",
      "[120]\ttrain-mlogloss:0.20973\teval-mlogloss:0.21049\n",
      "[121]\ttrain-mlogloss:0.20913\teval-mlogloss:0.21029\n",
      "[122]\ttrain-mlogloss:0.20867\teval-mlogloss:0.21006\n",
      "[123]\ttrain-mlogloss:0.20830\teval-mlogloss:0.21004\n",
      "[124]\ttrain-mlogloss:0.20787\teval-mlogloss:0.20991\n",
      "[125]\ttrain-mlogloss:0.20731\teval-mlogloss:0.20949\n",
      "[126]\ttrain-mlogloss:0.20675\teval-mlogloss:0.20948\n",
      "[127]\ttrain-mlogloss:0.20624\teval-mlogloss:0.20919\n",
      "[128]\ttrain-mlogloss:0.20576\teval-mlogloss:0.20900\n",
      "[129]\ttrain-mlogloss:0.20538\teval-mlogloss:0.20881\n",
      "[130]\ttrain-mlogloss:0.20495\teval-mlogloss:0.20875\n",
      "[131]\ttrain-mlogloss:0.20446\teval-mlogloss:0.20871\n",
      "[132]\ttrain-mlogloss:0.20408\teval-mlogloss:0.20861\n",
      "[133]\ttrain-mlogloss:0.20365\teval-mlogloss:0.20853\n",
      "[134]\ttrain-mlogloss:0.20308\teval-mlogloss:0.20844\n",
      "[135]\ttrain-mlogloss:0.20278\teval-mlogloss:0.20830\n",
      "[136]\ttrain-mlogloss:0.20229\teval-mlogloss:0.20828\n",
      "[137]\ttrain-mlogloss:0.20185\teval-mlogloss:0.20805\n",
      "[138]\ttrain-mlogloss:0.20124\teval-mlogloss:0.20799\n",
      "[139]\ttrain-mlogloss:0.20089\teval-mlogloss:0.20794\n",
      "[140]\ttrain-mlogloss:0.20051\teval-mlogloss:0.20780\n",
      "[141]\ttrain-mlogloss:0.20008\teval-mlogloss:0.20780\n",
      "[142]\ttrain-mlogloss:0.19955\teval-mlogloss:0.20745\n",
      "[143]\ttrain-mlogloss:0.19920\teval-mlogloss:0.20724\n",
      "[144]\ttrain-mlogloss:0.19875\teval-mlogloss:0.20714\n",
      "[145]\ttrain-mlogloss:0.19832\teval-mlogloss:0.20713\n",
      "[146]\ttrain-mlogloss:0.19784\teval-mlogloss:0.20706\n",
      "[147]\ttrain-mlogloss:0.19750\teval-mlogloss:0.20688\n",
      "[148]\ttrain-mlogloss:0.19711\teval-mlogloss:0.20691\n",
      "[149]\ttrain-mlogloss:0.19677\teval-mlogloss:0.20698\n",
      "[150]\ttrain-mlogloss:0.19627\teval-mlogloss:0.20689\n",
      "[151]\ttrain-mlogloss:0.19588\teval-mlogloss:0.20682\n",
      "[152]\ttrain-mlogloss:0.19568\teval-mlogloss:0.20674\n",
      "[153]\ttrain-mlogloss:0.19541\teval-mlogloss:0.20690\n",
      "[154]\ttrain-mlogloss:0.19498\teval-mlogloss:0.20688\n",
      "[155]\ttrain-mlogloss:0.19470\teval-mlogloss:0.20687\n",
      "[156]\ttrain-mlogloss:0.19417\teval-mlogloss:0.20683\n",
      "[157]\ttrain-mlogloss:0.19380\teval-mlogloss:0.20665\n",
      "[158]\ttrain-mlogloss:0.19336\teval-mlogloss:0.20667\n",
      "[159]\ttrain-mlogloss:0.19295\teval-mlogloss:0.20666\n",
      "[160]\ttrain-mlogloss:0.19258\teval-mlogloss:0.20679\n",
      "[161]\ttrain-mlogloss:0.19211\teval-mlogloss:0.20682\n",
      "[162]\ttrain-mlogloss:0.19181\teval-mlogloss:0.20680\n",
      "[163]\ttrain-mlogloss:0.19154\teval-mlogloss:0.20665\n",
      "[164]\ttrain-mlogloss:0.19122\teval-mlogloss:0.20688\n",
      "[165]\ttrain-mlogloss:0.19087\teval-mlogloss:0.20692\n",
      "[166]\ttrain-mlogloss:0.19058\teval-mlogloss:0.20672\n",
      "[167]\ttrain-mlogloss:0.19027\teval-mlogloss:0.20669\n",
      "[168]\ttrain-mlogloss:0.18991\teval-mlogloss:0.20670\n",
      "[169]\ttrain-mlogloss:0.18961\teval-mlogloss:0.20673\n",
      "[170]\ttrain-mlogloss:0.18924\teval-mlogloss:0.20695\n",
      "[171]\ttrain-mlogloss:0.18895\teval-mlogloss:0.20692\n",
      "[172]\ttrain-mlogloss:0.18868\teval-mlogloss:0.20686\n",
      "[173]\ttrain-mlogloss:0.18848\teval-mlogloss:0.20691\n",
      "[174]\ttrain-mlogloss:0.18821\teval-mlogloss:0.20704\n",
      "[175]\ttrain-mlogloss:0.18788\teval-mlogloss:0.20697\n",
      "[176]\ttrain-mlogloss:0.18765\teval-mlogloss:0.20707\n",
      "[177]\ttrain-mlogloss:0.18740\teval-mlogloss:0.20714\n",
      "[178]\ttrain-mlogloss:0.18703\teval-mlogloss:0.20711\n",
      "[179]\ttrain-mlogloss:0.18678\teval-mlogloss:0.20709\n",
      "[180]\ttrain-mlogloss:0.18657\teval-mlogloss:0.20709\n",
      "[181]\ttrain-mlogloss:0.18629\teval-mlogloss:0.20723\n",
      "[182]\ttrain-mlogloss:0.18597\teval-mlogloss:0.20723\n",
      "[183]\ttrain-mlogloss:0.18568\teval-mlogloss:0.20715\n",
      "[184]\ttrain-mlogloss:0.18524\teval-mlogloss:0.20711\n",
      "[185]\ttrain-mlogloss:0.18506\teval-mlogloss:0.20703\n",
      "[186]\ttrain-mlogloss:0.18472\teval-mlogloss:0.20699\n",
      "[187]\ttrain-mlogloss:0.18443\teval-mlogloss:0.20698\n",
      "[188]\ttrain-mlogloss:0.18411\teval-mlogloss:0.20684\n",
      "[189]\ttrain-mlogloss:0.18379\teval-mlogloss:0.20695\n",
      "[190]\ttrain-mlogloss:0.18334\teval-mlogloss:0.20705\n",
      "[191]\ttrain-mlogloss:0.18293\teval-mlogloss:0.20688\n",
      "[192]\ttrain-mlogloss:0.18252\teval-mlogloss:0.20686\n",
      "[193]\ttrain-mlogloss:0.18218\teval-mlogloss:0.20682\n",
      "[194]\ttrain-mlogloss:0.18202\teval-mlogloss:0.20676\n",
      "[195]\ttrain-mlogloss:0.18178\teval-mlogloss:0.20677\n",
      "[196]\ttrain-mlogloss:0.18144\teval-mlogloss:0.20664\n",
      "[197]\ttrain-mlogloss:0.18101\teval-mlogloss:0.20681\n",
      "[198]\ttrain-mlogloss:0.18076\teval-mlogloss:0.20680\n",
      "[199]\ttrain-mlogloss:0.18046\teval-mlogloss:0.20685\n",
      "[200]\ttrain-mlogloss:0.18022\teval-mlogloss:0.20691\n",
      "[201]\ttrain-mlogloss:0.17987\teval-mlogloss:0.20711\n",
      "[202]\ttrain-mlogloss:0.17966\teval-mlogloss:0.20706\n",
      "[203]\ttrain-mlogloss:0.17936\teval-mlogloss:0.20678\n",
      "[204]\ttrain-mlogloss:0.17902\teval-mlogloss:0.20691\n",
      "[205]\ttrain-mlogloss:0.17875\teval-mlogloss:0.20695\n",
      "[206]\ttrain-mlogloss:0.17839\teval-mlogloss:0.20678\n",
      "[207]\ttrain-mlogloss:0.17809\teval-mlogloss:0.20678\n",
      "[208]\ttrain-mlogloss:0.17785\teval-mlogloss:0.20673\n",
      "[209]\ttrain-mlogloss:0.17756\teval-mlogloss:0.20674\n",
      "[210]\ttrain-mlogloss:0.17726\teval-mlogloss:0.20680\n",
      "[211]\ttrain-mlogloss:0.17705\teval-mlogloss:0.20679\n",
      "[212]\ttrain-mlogloss:0.17677\teval-mlogloss:0.20676\n",
      "[213]\ttrain-mlogloss:0.17644\teval-mlogloss:0.20664\n",
      "[214]\ttrain-mlogloss:0.17619\teval-mlogloss:0.20652\n",
      "[215]\ttrain-mlogloss:0.17586\teval-mlogloss:0.20653\n",
      "[216]\ttrain-mlogloss:0.17558\teval-mlogloss:0.20672\n",
      "[217]\ttrain-mlogloss:0.17540\teval-mlogloss:0.20683\n",
      "[218]\ttrain-mlogloss:0.17516\teval-mlogloss:0.20680\n",
      "[219]\ttrain-mlogloss:0.17490\teval-mlogloss:0.20698\n",
      "[220]\ttrain-mlogloss:0.17455\teval-mlogloss:0.20678\n",
      "[221]\ttrain-mlogloss:0.17430\teval-mlogloss:0.20675\n",
      "[222]\ttrain-mlogloss:0.17408\teval-mlogloss:0.20676\n",
      "[223]\ttrain-mlogloss:0.17380\teval-mlogloss:0.20677\n",
      "[224]\ttrain-mlogloss:0.17350\teval-mlogloss:0.20692\n",
      "[225]\ttrain-mlogloss:0.17324\teval-mlogloss:0.20710\n",
      "[226]\ttrain-mlogloss:0.17281\teval-mlogloss:0.20713\n",
      "[227]\ttrain-mlogloss:0.17246\teval-mlogloss:0.20715\n",
      "[228]\ttrain-mlogloss:0.17211\teval-mlogloss:0.20688\n",
      "[229]\ttrain-mlogloss:0.17172\teval-mlogloss:0.20706\n",
      "[230]\ttrain-mlogloss:0.17147\teval-mlogloss:0.20702\n",
      "[231]\ttrain-mlogloss:0.17118\teval-mlogloss:0.20713\n",
      "[232]\ttrain-mlogloss:0.17094\teval-mlogloss:0.20718\n",
      "[233]\ttrain-mlogloss:0.17058\teval-mlogloss:0.20715\n",
      "[234]\ttrain-mlogloss:0.17035\teval-mlogloss:0.20709\n",
      "[235]\ttrain-mlogloss:0.17016\teval-mlogloss:0.20724\n",
      "[236]\ttrain-mlogloss:0.16984\teval-mlogloss:0.20741\n",
      "[237]\ttrain-mlogloss:0.16957\teval-mlogloss:0.20740\n",
      "[238]\ttrain-mlogloss:0.16920\teval-mlogloss:0.20752\n",
      "[239]\ttrain-mlogloss:0.16895\teval-mlogloss:0.20765\n",
      "[240]\ttrain-mlogloss:0.16861\teval-mlogloss:0.20769\n",
      "[241]\ttrain-mlogloss:0.16845\teval-mlogloss:0.20776\n",
      "[242]\ttrain-mlogloss:0.16811\teval-mlogloss:0.20785\n",
      "[243]\ttrain-mlogloss:0.16794\teval-mlogloss:0.20791\n",
      "[244]\ttrain-mlogloss:0.16782\teval-mlogloss:0.20792\n",
      "[245]\ttrain-mlogloss:0.16765\teval-mlogloss:0.20802\n",
      "[246]\ttrain-mlogloss:0.16739\teval-mlogloss:0.20807\n",
      "[247]\ttrain-mlogloss:0.16723\teval-mlogloss:0.20809\n",
      "[248]\ttrain-mlogloss:0.16704\teval-mlogloss:0.20811\n",
      "[249]\ttrain-mlogloss:0.16684\teval-mlogloss:0.20821\n",
      "[250]\ttrain-mlogloss:0.16651\teval-mlogloss:0.20829\n",
      "[251]\ttrain-mlogloss:0.16632\teval-mlogloss:0.20836\n",
      "[252]\ttrain-mlogloss:0.16603\teval-mlogloss:0.20835\n",
      "[253]\ttrain-mlogloss:0.16578\teval-mlogloss:0.20834\n",
      "[254]\ttrain-mlogloss:0.16557\teval-mlogloss:0.20834\n",
      "[255]\ttrain-mlogloss:0.16533\teval-mlogloss:0.20822\n",
      "[256]\ttrain-mlogloss:0.16506\teval-mlogloss:0.20810\n",
      "[257]\ttrain-mlogloss:0.16479\teval-mlogloss:0.20800\n",
      "[258]\ttrain-mlogloss:0.16447\teval-mlogloss:0.20795\n",
      "[259]\ttrain-mlogloss:0.16440\teval-mlogloss:0.20800\n",
      "[260]\ttrain-mlogloss:0.16423\teval-mlogloss:0.20800\n",
      "[261]\ttrain-mlogloss:0.16400\teval-mlogloss:0.20801\n",
      "[262]\ttrain-mlogloss:0.16383\teval-mlogloss:0.20804\n",
      "[263]\ttrain-mlogloss:0.16359\teval-mlogloss:0.20806\n",
      "[264]\ttrain-mlogloss:0.16338\teval-mlogloss:0.20798\n",
      "[265]\ttrain-mlogloss:0.16306\teval-mlogloss:0.20807\n",
      "[266]\ttrain-mlogloss:0.16284\teval-mlogloss:0.20809\n",
      "[267]\ttrain-mlogloss:0.16258\teval-mlogloss:0.20826\n",
      "[268]\ttrain-mlogloss:0.16240\teval-mlogloss:0.20826\n",
      "[269]\ttrain-mlogloss:0.16220\teval-mlogloss:0.20832\n",
      "[270]\ttrain-mlogloss:0.16202\teval-mlogloss:0.20832\n",
      "[271]\ttrain-mlogloss:0.16189\teval-mlogloss:0.20834\n",
      "[272]\ttrain-mlogloss:0.16168\teval-mlogloss:0.20829\n",
      "[273]\ttrain-mlogloss:0.16149\teval-mlogloss:0.20819\n",
      "[274]\ttrain-mlogloss:0.16130\teval-mlogloss:0.20834\n",
      "[275]\ttrain-mlogloss:0.16114\teval-mlogloss:0.20830\n",
      "[276]\ttrain-mlogloss:0.16097\teval-mlogloss:0.20843\n",
      "[277]\ttrain-mlogloss:0.16078\teval-mlogloss:0.20837\n",
      "[278]\ttrain-mlogloss:0.16058\teval-mlogloss:0.20844\n",
      "[279]\ttrain-mlogloss:0.16044\teval-mlogloss:0.20848\n",
      "[280]\ttrain-mlogloss:0.16035\teval-mlogloss:0.20852\n",
      "[281]\ttrain-mlogloss:0.16013\teval-mlogloss:0.20855\n",
      "[282]\ttrain-mlogloss:0.15986\teval-mlogloss:0.20871\n",
      "[283]\ttrain-mlogloss:0.15963\teval-mlogloss:0.20871\n",
      "[284]\ttrain-mlogloss:0.15942\teval-mlogloss:0.20849\n",
      "[285]\ttrain-mlogloss:0.15926\teval-mlogloss:0.20844\n",
      "[286]\ttrain-mlogloss:0.15895\teval-mlogloss:0.20859\n",
      "[287]\ttrain-mlogloss:0.15879\teval-mlogloss:0.20852\n",
      "[288]\ttrain-mlogloss:0.15859\teval-mlogloss:0.20861\n",
      "[289]\ttrain-mlogloss:0.15847\teval-mlogloss:0.20863\n",
      "[290]\ttrain-mlogloss:0.15813\teval-mlogloss:0.20867\n",
      "[291]\ttrain-mlogloss:0.15792\teval-mlogloss:0.20871\n",
      "[292]\ttrain-mlogloss:0.15780\teval-mlogloss:0.20867\n",
      "[293]\ttrain-mlogloss:0.15759\teval-mlogloss:0.20869\n",
      "[294]\ttrain-mlogloss:0.15739\teval-mlogloss:0.20879\n",
      "[295]\ttrain-mlogloss:0.15719\teval-mlogloss:0.20876\n",
      "[296]\ttrain-mlogloss:0.15711\teval-mlogloss:0.20879\n",
      "[297]\ttrain-mlogloss:0.15702\teval-mlogloss:0.20885\n",
      "[298]\ttrain-mlogloss:0.15686\teval-mlogloss:0.20896\n",
      "[299]\ttrain-mlogloss:0.15667\teval-mlogloss:0.20884\n",
      "[300]\ttrain-mlogloss:0.15652\teval-mlogloss:0.20908\n",
      "[301]\ttrain-mlogloss:0.15637\teval-mlogloss:0.20910\n",
      "[302]\ttrain-mlogloss:0.15628\teval-mlogloss:0.20903\n",
      "[303]\ttrain-mlogloss:0.15611\teval-mlogloss:0.20897\n",
      "[304]\ttrain-mlogloss:0.15595\teval-mlogloss:0.20905\n",
      "[305]\ttrain-mlogloss:0.15576\teval-mlogloss:0.20915\n",
      "[306]\ttrain-mlogloss:0.15566\teval-mlogloss:0.20922\n",
      "[307]\ttrain-mlogloss:0.15555\teval-mlogloss:0.20927\n",
      "[308]\ttrain-mlogloss:0.15536\teval-mlogloss:0.20920\n",
      "[309]\ttrain-mlogloss:0.15511\teval-mlogloss:0.20907\n",
      "[310]\ttrain-mlogloss:0.15496\teval-mlogloss:0.20909\n",
      "[311]\ttrain-mlogloss:0.15479\teval-mlogloss:0.20919\n",
      "[312]\ttrain-mlogloss:0.15473\teval-mlogloss:0.20919\n",
      "[313]\ttrain-mlogloss:0.15464\teval-mlogloss:0.20916\n",
      "[314]\ttrain-mlogloss:0.15440\teval-mlogloss:0.20928\n",
      "xgb now score is: [2.5102825116552414, 2.2800363773293792, 2.366547712795436, 2.7053648946527393]\n",
      "[0]\ttrain-mlogloss:0.67129\teval-mlogloss:0.67080\n",
      "[1]\ttrain-mlogloss:0.65071\teval-mlogloss:0.64987\n",
      "[2]\ttrain-mlogloss:0.63138\teval-mlogloss:0.63042\n",
      "[3]\ttrain-mlogloss:0.61287\teval-mlogloss:0.61182\n",
      "[4]\ttrain-mlogloss:0.59513\teval-mlogloss:0.59377\n",
      "[5]\ttrain-mlogloss:0.57857\teval-mlogloss:0.57697\n",
      "[6]\ttrain-mlogloss:0.56280\teval-mlogloss:0.56080\n",
      "[7]\ttrain-mlogloss:0.54772\teval-mlogloss:0.54552\n",
      "[8]\ttrain-mlogloss:0.53336\teval-mlogloss:0.53107\n",
      "[9]\ttrain-mlogloss:0.51999\teval-mlogloss:0.51761\n",
      "[10]\ttrain-mlogloss:0.50729\teval-mlogloss:0.50460\n",
      "[11]\ttrain-mlogloss:0.49533\teval-mlogloss:0.49231\n",
      "[12]\ttrain-mlogloss:0.48390\teval-mlogloss:0.48065\n",
      "[13]\ttrain-mlogloss:0.47298\teval-mlogloss:0.46946\n",
      "[14]\ttrain-mlogloss:0.46254\teval-mlogloss:0.45863\n",
      "[15]\ttrain-mlogloss:0.45247\teval-mlogloss:0.44851\n",
      "[16]\ttrain-mlogloss:0.44294\teval-mlogloss:0.43862\n",
      "[17]\ttrain-mlogloss:0.43380\teval-mlogloss:0.42920\n",
      "[18]\ttrain-mlogloss:0.42502\teval-mlogloss:0.42029\n",
      "[19]\ttrain-mlogloss:0.41674\teval-mlogloss:0.41191\n",
      "[20]\ttrain-mlogloss:0.40869\teval-mlogloss:0.40382\n",
      "[21]\ttrain-mlogloss:0.40092\teval-mlogloss:0.39598\n",
      "[22]\ttrain-mlogloss:0.39366\teval-mlogloss:0.38860\n",
      "[23]\ttrain-mlogloss:0.38661\teval-mlogloss:0.38138\n",
      "[24]\ttrain-mlogloss:0.37988\teval-mlogloss:0.37452\n",
      "[25]\ttrain-mlogloss:0.37350\teval-mlogloss:0.36792\n",
      "[26]\ttrain-mlogloss:0.36738\teval-mlogloss:0.36175\n",
      "[27]\ttrain-mlogloss:0.36141\teval-mlogloss:0.35556\n",
      "[28]\ttrain-mlogloss:0.35581\teval-mlogloss:0.34988\n",
      "[29]\ttrain-mlogloss:0.35037\teval-mlogloss:0.34438\n",
      "[30]\ttrain-mlogloss:0.34515\teval-mlogloss:0.33907\n",
      "[31]\ttrain-mlogloss:0.34004\teval-mlogloss:0.33392\n",
      "[32]\ttrain-mlogloss:0.33538\teval-mlogloss:0.32914\n",
      "[33]\ttrain-mlogloss:0.33063\teval-mlogloss:0.32419\n",
      "[34]\ttrain-mlogloss:0.32632\teval-mlogloss:0.31988\n",
      "[35]\ttrain-mlogloss:0.32216\teval-mlogloss:0.31573\n",
      "[36]\ttrain-mlogloss:0.31802\teval-mlogloss:0.31158\n",
      "[37]\ttrain-mlogloss:0.31424\teval-mlogloss:0.30770\n",
      "[38]\ttrain-mlogloss:0.31061\teval-mlogloss:0.30386\n",
      "[39]\ttrain-mlogloss:0.30706\teval-mlogloss:0.30023\n",
      "[40]\ttrain-mlogloss:0.30361\teval-mlogloss:0.29666\n",
      "[41]\ttrain-mlogloss:0.30032\teval-mlogloss:0.29336\n",
      "[42]\ttrain-mlogloss:0.29722\teval-mlogloss:0.29018\n",
      "[43]\ttrain-mlogloss:0.29414\teval-mlogloss:0.28711\n",
      "[44]\ttrain-mlogloss:0.29124\teval-mlogloss:0.28419\n",
      "[45]\ttrain-mlogloss:0.28836\teval-mlogloss:0.28129\n",
      "[46]\ttrain-mlogloss:0.28558\teval-mlogloss:0.27864\n",
      "[47]\ttrain-mlogloss:0.28294\teval-mlogloss:0.27589\n",
      "[48]\ttrain-mlogloss:0.28051\teval-mlogloss:0.27346\n",
      "[49]\ttrain-mlogloss:0.27809\teval-mlogloss:0.27120\n",
      "[50]\ttrain-mlogloss:0.27573\teval-mlogloss:0.26877\n",
      "[51]\ttrain-mlogloss:0.27339\teval-mlogloss:0.26661\n",
      "[52]\ttrain-mlogloss:0.27134\teval-mlogloss:0.26468\n",
      "[53]\ttrain-mlogloss:0.26913\teval-mlogloss:0.26267\n",
      "[54]\ttrain-mlogloss:0.26712\teval-mlogloss:0.26076\n",
      "[55]\ttrain-mlogloss:0.26524\teval-mlogloss:0.25890\n",
      "[56]\ttrain-mlogloss:0.26334\teval-mlogloss:0.25685\n",
      "[57]\ttrain-mlogloss:0.26147\teval-mlogloss:0.25494\n",
      "[58]\ttrain-mlogloss:0.25958\teval-mlogloss:0.25314\n",
      "[59]\ttrain-mlogloss:0.25788\teval-mlogloss:0.25155\n",
      "[60]\ttrain-mlogloss:0.25620\teval-mlogloss:0.24990\n",
      "[61]\ttrain-mlogloss:0.25463\teval-mlogloss:0.24843\n",
      "[62]\ttrain-mlogloss:0.25316\teval-mlogloss:0.24705\n",
      "[63]\ttrain-mlogloss:0.25172\teval-mlogloss:0.24565\n",
      "[64]\ttrain-mlogloss:0.25020\teval-mlogloss:0.24420\n",
      "[65]\ttrain-mlogloss:0.24888\teval-mlogloss:0.24298\n",
      "[66]\ttrain-mlogloss:0.24743\teval-mlogloss:0.24172\n",
      "[67]\ttrain-mlogloss:0.24628\teval-mlogloss:0.24053\n",
      "[68]\ttrain-mlogloss:0.24506\teval-mlogloss:0.23944\n",
      "[69]\ttrain-mlogloss:0.24394\teval-mlogloss:0.23841\n",
      "[70]\ttrain-mlogloss:0.24285\teval-mlogloss:0.23744\n",
      "[71]\ttrain-mlogloss:0.24177\teval-mlogloss:0.23644\n",
      "[72]\ttrain-mlogloss:0.24081\teval-mlogloss:0.23558\n",
      "[73]\ttrain-mlogloss:0.23990\teval-mlogloss:0.23471\n",
      "[74]\ttrain-mlogloss:0.23902\teval-mlogloss:0.23393\n",
      "[75]\ttrain-mlogloss:0.23797\teval-mlogloss:0.23300\n",
      "[76]\ttrain-mlogloss:0.23684\teval-mlogloss:0.23223\n",
      "[77]\ttrain-mlogloss:0.23590\teval-mlogloss:0.23157\n",
      "[78]\ttrain-mlogloss:0.23500\teval-mlogloss:0.23088\n",
      "[79]\ttrain-mlogloss:0.23404\teval-mlogloss:0.23032\n",
      "[80]\ttrain-mlogloss:0.23308\teval-mlogloss:0.22942\n",
      "[81]\ttrain-mlogloss:0.23229\teval-mlogloss:0.22875\n",
      "[82]\ttrain-mlogloss:0.23138\teval-mlogloss:0.22821\n",
      "[83]\ttrain-mlogloss:0.23067\teval-mlogloss:0.22768\n",
      "[84]\ttrain-mlogloss:0.22972\teval-mlogloss:0.22691\n",
      "[85]\ttrain-mlogloss:0.22909\teval-mlogloss:0.22637\n",
      "[86]\ttrain-mlogloss:0.22820\teval-mlogloss:0.22592\n",
      "[87]\ttrain-mlogloss:0.22742\teval-mlogloss:0.22531\n",
      "[88]\ttrain-mlogloss:0.22667\teval-mlogloss:0.22481\n",
      "[89]\ttrain-mlogloss:0.22593\teval-mlogloss:0.22415\n",
      "[90]\ttrain-mlogloss:0.22528\teval-mlogloss:0.22385\n",
      "[91]\ttrain-mlogloss:0.22466\teval-mlogloss:0.22342\n",
      "[92]\ttrain-mlogloss:0.22394\teval-mlogloss:0.22317\n",
      "[93]\ttrain-mlogloss:0.22331\teval-mlogloss:0.22273\n",
      "[94]\ttrain-mlogloss:0.22260\teval-mlogloss:0.22237\n",
      "[95]\ttrain-mlogloss:0.22201\teval-mlogloss:0.22183\n",
      "[96]\ttrain-mlogloss:0.22146\teval-mlogloss:0.22153\n",
      "[97]\ttrain-mlogloss:0.22085\teval-mlogloss:0.22121\n",
      "[98]\ttrain-mlogloss:0.22030\teval-mlogloss:0.22086\n",
      "[99]\ttrain-mlogloss:0.21982\teval-mlogloss:0.22048\n",
      "[100]\ttrain-mlogloss:0.21928\teval-mlogloss:0.22024\n",
      "[101]\ttrain-mlogloss:0.21877\teval-mlogloss:0.22003\n",
      "[102]\ttrain-mlogloss:0.21818\teval-mlogloss:0.21983\n",
      "[103]\ttrain-mlogloss:0.21755\teval-mlogloss:0.21944\n",
      "[104]\ttrain-mlogloss:0.21707\teval-mlogloss:0.21936\n",
      "[105]\ttrain-mlogloss:0.21652\teval-mlogloss:0.21911\n",
      "[106]\ttrain-mlogloss:0.21599\teval-mlogloss:0.21884\n",
      "[107]\ttrain-mlogloss:0.21538\teval-mlogloss:0.21858\n",
      "[108]\ttrain-mlogloss:0.21479\teval-mlogloss:0.21829\n",
      "[109]\ttrain-mlogloss:0.21421\teval-mlogloss:0.21819\n",
      "[110]\ttrain-mlogloss:0.21359\teval-mlogloss:0.21802\n",
      "[111]\ttrain-mlogloss:0.21305\teval-mlogloss:0.21778\n",
      "[112]\ttrain-mlogloss:0.21245\teval-mlogloss:0.21763\n",
      "[113]\ttrain-mlogloss:0.21188\teval-mlogloss:0.21747\n",
      "[114]\ttrain-mlogloss:0.21138\teval-mlogloss:0.21715\n",
      "[115]\ttrain-mlogloss:0.21099\teval-mlogloss:0.21717\n",
      "[116]\ttrain-mlogloss:0.21050\teval-mlogloss:0.21705\n",
      "[117]\ttrain-mlogloss:0.20998\teval-mlogloss:0.21666\n",
      "[118]\ttrain-mlogloss:0.20955\teval-mlogloss:0.21648\n",
      "[119]\ttrain-mlogloss:0.20901\teval-mlogloss:0.21650\n",
      "[120]\ttrain-mlogloss:0.20840\teval-mlogloss:0.21643\n",
      "[121]\ttrain-mlogloss:0.20794\teval-mlogloss:0.21619\n",
      "[122]\ttrain-mlogloss:0.20759\teval-mlogloss:0.21594\n",
      "[123]\ttrain-mlogloss:0.20707\teval-mlogloss:0.21587\n",
      "[124]\ttrain-mlogloss:0.20662\teval-mlogloss:0.21589\n",
      "[125]\ttrain-mlogloss:0.20604\teval-mlogloss:0.21553\n",
      "[126]\ttrain-mlogloss:0.20549\teval-mlogloss:0.21558\n",
      "[127]\ttrain-mlogloss:0.20498\teval-mlogloss:0.21539\n",
      "[128]\ttrain-mlogloss:0.20454\teval-mlogloss:0.21532\n",
      "[129]\ttrain-mlogloss:0.20392\teval-mlogloss:0.21530\n",
      "[130]\ttrain-mlogloss:0.20347\teval-mlogloss:0.21520\n",
      "[131]\ttrain-mlogloss:0.20307\teval-mlogloss:0.21508\n",
      "[132]\ttrain-mlogloss:0.20267\teval-mlogloss:0.21484\n",
      "[133]\ttrain-mlogloss:0.20204\teval-mlogloss:0.21466\n",
      "[134]\ttrain-mlogloss:0.20141\teval-mlogloss:0.21461\n",
      "[135]\ttrain-mlogloss:0.20093\teval-mlogloss:0.21457\n",
      "[136]\ttrain-mlogloss:0.20050\teval-mlogloss:0.21453\n",
      "[137]\ttrain-mlogloss:0.20017\teval-mlogloss:0.21450\n",
      "[138]\ttrain-mlogloss:0.19973\teval-mlogloss:0.21468\n",
      "[139]\ttrain-mlogloss:0.19933\teval-mlogloss:0.21471\n",
      "[140]\ttrain-mlogloss:0.19893\teval-mlogloss:0.21448\n",
      "[141]\ttrain-mlogloss:0.19847\teval-mlogloss:0.21451\n",
      "[142]\ttrain-mlogloss:0.19809\teval-mlogloss:0.21441\n",
      "[143]\ttrain-mlogloss:0.19763\teval-mlogloss:0.21446\n",
      "[144]\ttrain-mlogloss:0.19729\teval-mlogloss:0.21441\n",
      "[145]\ttrain-mlogloss:0.19707\teval-mlogloss:0.21453\n",
      "[146]\ttrain-mlogloss:0.19666\teval-mlogloss:0.21437\n",
      "[147]\ttrain-mlogloss:0.19630\teval-mlogloss:0.21426\n",
      "[148]\ttrain-mlogloss:0.19588\teval-mlogloss:0.21398\n",
      "[149]\ttrain-mlogloss:0.19541\teval-mlogloss:0.21404\n",
      "[150]\ttrain-mlogloss:0.19513\teval-mlogloss:0.21406\n",
      "[151]\ttrain-mlogloss:0.19475\teval-mlogloss:0.21424\n",
      "[152]\ttrain-mlogloss:0.19427\teval-mlogloss:0.21422\n",
      "[153]\ttrain-mlogloss:0.19384\teval-mlogloss:0.21397\n",
      "[154]\ttrain-mlogloss:0.19342\teval-mlogloss:0.21404\n",
      "[155]\ttrain-mlogloss:0.19304\teval-mlogloss:0.21393\n",
      "[156]\ttrain-mlogloss:0.19264\teval-mlogloss:0.21392\n",
      "[157]\ttrain-mlogloss:0.19241\teval-mlogloss:0.21397\n",
      "[158]\ttrain-mlogloss:0.19205\teval-mlogloss:0.21407\n",
      "[159]\ttrain-mlogloss:0.19170\teval-mlogloss:0.21399\n",
      "[160]\ttrain-mlogloss:0.19136\teval-mlogloss:0.21407\n",
      "[161]\ttrain-mlogloss:0.19094\teval-mlogloss:0.21395\n",
      "[162]\ttrain-mlogloss:0.19062\teval-mlogloss:0.21402\n",
      "[163]\ttrain-mlogloss:0.19023\teval-mlogloss:0.21405\n",
      "[164]\ttrain-mlogloss:0.19004\teval-mlogloss:0.21406\n",
      "[165]\ttrain-mlogloss:0.18982\teval-mlogloss:0.21415\n",
      "[166]\ttrain-mlogloss:0.18950\teval-mlogloss:0.21420\n",
      "[167]\ttrain-mlogloss:0.18926\teval-mlogloss:0.21427\n",
      "[168]\ttrain-mlogloss:0.18904\teval-mlogloss:0.21427\n",
      "[169]\ttrain-mlogloss:0.18866\teval-mlogloss:0.21424\n",
      "[170]\ttrain-mlogloss:0.18843\teval-mlogloss:0.21435\n",
      "[171]\ttrain-mlogloss:0.18801\teval-mlogloss:0.21436\n",
      "[172]\ttrain-mlogloss:0.18768\teval-mlogloss:0.21431\n",
      "[173]\ttrain-mlogloss:0.18742\teval-mlogloss:0.21425\n",
      "[174]\ttrain-mlogloss:0.18700\teval-mlogloss:0.21422\n",
      "[175]\ttrain-mlogloss:0.18665\teval-mlogloss:0.21418\n",
      "[176]\ttrain-mlogloss:0.18634\teval-mlogloss:0.21441\n",
      "[177]\ttrain-mlogloss:0.18597\teval-mlogloss:0.21451\n",
      "[178]\ttrain-mlogloss:0.18560\teval-mlogloss:0.21444\n",
      "[179]\ttrain-mlogloss:0.18530\teval-mlogloss:0.21464\n",
      "[180]\ttrain-mlogloss:0.18502\teval-mlogloss:0.21485\n",
      "[181]\ttrain-mlogloss:0.18463\teval-mlogloss:0.21493\n",
      "[182]\ttrain-mlogloss:0.18427\teval-mlogloss:0.21482\n",
      "[183]\ttrain-mlogloss:0.18408\teval-mlogloss:0.21489\n",
      "[184]\ttrain-mlogloss:0.18374\teval-mlogloss:0.21487\n",
      "[185]\ttrain-mlogloss:0.18345\teval-mlogloss:0.21496\n",
      "[186]\ttrain-mlogloss:0.18317\teval-mlogloss:0.21491\n",
      "[187]\ttrain-mlogloss:0.18282\teval-mlogloss:0.21508\n",
      "[188]\ttrain-mlogloss:0.18240\teval-mlogloss:0.21504\n",
      "[189]\ttrain-mlogloss:0.18222\teval-mlogloss:0.21505\n",
      "[190]\ttrain-mlogloss:0.18196\teval-mlogloss:0.21512\n",
      "[191]\ttrain-mlogloss:0.18168\teval-mlogloss:0.21534\n",
      "[192]\ttrain-mlogloss:0.18137\teval-mlogloss:0.21532\n",
      "[193]\ttrain-mlogloss:0.18093\teval-mlogloss:0.21505\n",
      "[194]\ttrain-mlogloss:0.18082\teval-mlogloss:0.21509\n",
      "[195]\ttrain-mlogloss:0.18061\teval-mlogloss:0.21523\n",
      "[196]\ttrain-mlogloss:0.18035\teval-mlogloss:0.21526\n",
      "[197]\ttrain-mlogloss:0.18007\teval-mlogloss:0.21541\n",
      "[198]\ttrain-mlogloss:0.17990\teval-mlogloss:0.21540\n",
      "[199]\ttrain-mlogloss:0.17970\teval-mlogloss:0.21543\n",
      "[200]\ttrain-mlogloss:0.17932\teval-mlogloss:0.21544\n",
      "[201]\ttrain-mlogloss:0.17899\teval-mlogloss:0.21545\n",
      "[202]\ttrain-mlogloss:0.17872\teval-mlogloss:0.21532\n",
      "[203]\ttrain-mlogloss:0.17856\teval-mlogloss:0.21517\n",
      "[204]\ttrain-mlogloss:0.17830\teval-mlogloss:0.21514\n",
      "[205]\ttrain-mlogloss:0.17797\teval-mlogloss:0.21516\n",
      "[206]\ttrain-mlogloss:0.17774\teval-mlogloss:0.21533\n",
      "[207]\ttrain-mlogloss:0.17743\teval-mlogloss:0.21548\n",
      "[208]\ttrain-mlogloss:0.17713\teval-mlogloss:0.21566\n",
      "[209]\ttrain-mlogloss:0.17684\teval-mlogloss:0.21580\n",
      "[210]\ttrain-mlogloss:0.17655\teval-mlogloss:0.21593\n",
      "[211]\ttrain-mlogloss:0.17631\teval-mlogloss:0.21598\n",
      "[212]\ttrain-mlogloss:0.17597\teval-mlogloss:0.21597\n",
      "[213]\ttrain-mlogloss:0.17563\teval-mlogloss:0.21590\n",
      "[214]\ttrain-mlogloss:0.17538\teval-mlogloss:0.21593\n",
      "[215]\ttrain-mlogloss:0.17512\teval-mlogloss:0.21588\n",
      "[216]\ttrain-mlogloss:0.17489\teval-mlogloss:0.21591\n",
      "[217]\ttrain-mlogloss:0.17468\teval-mlogloss:0.21566\n",
      "[218]\ttrain-mlogloss:0.17447\teval-mlogloss:0.21567\n",
      "[219]\ttrain-mlogloss:0.17420\teval-mlogloss:0.21584\n",
      "[220]\ttrain-mlogloss:0.17403\teval-mlogloss:0.21588\n",
      "[221]\ttrain-mlogloss:0.17380\teval-mlogloss:0.21593\n",
      "[222]\ttrain-mlogloss:0.17350\teval-mlogloss:0.21586\n",
      "[223]\ttrain-mlogloss:0.17328\teval-mlogloss:0.21587\n",
      "[224]\ttrain-mlogloss:0.17290\teval-mlogloss:0.21598\n",
      "[225]\ttrain-mlogloss:0.17263\teval-mlogloss:0.21595\n",
      "[226]\ttrain-mlogloss:0.17250\teval-mlogloss:0.21600\n",
      "[227]\ttrain-mlogloss:0.17220\teval-mlogloss:0.21604\n",
      "[228]\ttrain-mlogloss:0.17206\teval-mlogloss:0.21610\n",
      "[229]\ttrain-mlogloss:0.17170\teval-mlogloss:0.21624\n",
      "[230]\ttrain-mlogloss:0.17156\teval-mlogloss:0.21622\n",
      "[231]\ttrain-mlogloss:0.17137\teval-mlogloss:0.21631\n",
      "[232]\ttrain-mlogloss:0.17111\teval-mlogloss:0.21634\n",
      "[233]\ttrain-mlogloss:0.17086\teval-mlogloss:0.21630\n",
      "[234]\ttrain-mlogloss:0.17059\teval-mlogloss:0.21633\n",
      "[235]\ttrain-mlogloss:0.17036\teval-mlogloss:0.21621\n",
      "[236]\ttrain-mlogloss:0.17013\teval-mlogloss:0.21644\n",
      "[237]\ttrain-mlogloss:0.16979\teval-mlogloss:0.21653\n",
      "[238]\ttrain-mlogloss:0.16955\teval-mlogloss:0.21678\n",
      "[239]\ttrain-mlogloss:0.16923\teval-mlogloss:0.21669\n",
      "[240]\ttrain-mlogloss:0.16900\teval-mlogloss:0.21663\n",
      "[241]\ttrain-mlogloss:0.16875\teval-mlogloss:0.21649\n",
      "[242]\ttrain-mlogloss:0.16855\teval-mlogloss:0.21645\n",
      "[243]\ttrain-mlogloss:0.16849\teval-mlogloss:0.21638\n",
      "[244]\ttrain-mlogloss:0.16830\teval-mlogloss:0.21645\n",
      "[245]\ttrain-mlogloss:0.16803\teval-mlogloss:0.21652\n",
      "[246]\ttrain-mlogloss:0.16767\teval-mlogloss:0.21671\n",
      "[247]\ttrain-mlogloss:0.16754\teval-mlogloss:0.21676\n",
      "[248]\ttrain-mlogloss:0.16738\teval-mlogloss:0.21668\n",
      "[249]\ttrain-mlogloss:0.16715\teval-mlogloss:0.21668\n",
      "[250]\ttrain-mlogloss:0.16684\teval-mlogloss:0.21669\n",
      "[251]\ttrain-mlogloss:0.16653\teval-mlogloss:0.21656\n",
      "[252]\ttrain-mlogloss:0.16613\teval-mlogloss:0.21659\n",
      "[253]\ttrain-mlogloss:0.16582\teval-mlogloss:0.21688\n",
      "[254]\ttrain-mlogloss:0.16563\teval-mlogloss:0.21697\n",
      "[255]\ttrain-mlogloss:0.16530\teval-mlogloss:0.21701\n",
      "[256]\ttrain-mlogloss:0.16506\teval-mlogloss:0.21696\n",
      "xgb now score is: [2.5102825116552414, 2.2800363773293792, 2.366547712795436, 2.7053648946527393, 2.655002480186522]\n",
      "xgb_score_list: [2.5102825116552414, 2.2800363773293792, 2.366547712795436, 2.7053648946527393, 2.655002480186522]\n",
      "xgb_score_mean: 2.503446795323863\n"
     ]
    }
   ],
   "source": [
    "clf_list = [lgb_clf, xgb_clf]\n",
    "column_list = []\n",
    "train_data_list=[]\n",
    "test_data_list=[]\n",
    "for clf in clf_list:\n",
    "    train_data,test_data,clf_name=clf(x_train, y_train, x_valid, kf, label_split=None)\n",
    "    train_data_list.append(train_data)\n",
    "    test_data_list.append(test_data)\n",
    "train_stacking = np.concatenate(train_data_list, axis=1)\n",
    "test_stacking = np.concatenate(test_data_list, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [
    "train = pd.DataFrame(np.concatenate([x_train, train_stacking], axis=1))\n",
    "test = np.concatenate([x_valid, test_stacking], axis=1)\n",
    "\n",
    "df_train_all = pd.DataFrame(train)\n",
    "df_train_all.columns = features_columns + clf_list_col\n",
    "df_test_all = pd.DataFrame(test)\n",
    "df_test_all.columns = features_columns + clf_list_col\n",
    "\n",
    "df_train_all['user_id'] = all_data_test[~all_data_test['label'].isna()]['user_id']\n",
    "df_test_all['user_id'] = all_data_test[all_data_test['label'].isna()]['user_id']\n",
    "df_train_all['label'] = all_data_test[~all_data_test['label'].isna()]['label']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [],
   "source": [
    "df_train_all.to_csv(\"data/train_all.csv\", header=True, index=False)\n",
    "df_test_all.to_csv(\"data/test_all.csv\", header=True, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}