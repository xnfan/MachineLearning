{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "import gc\n",
    "from collections import Counter\n",
    "import copy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"./data/test_format1.csv\")\n",
    "train_data = pd.read_csv(\"./data/train_format1.csv\")\n",
    "user_info = pd.read_csv(\"./data/user_info_format1.csv\")\n",
    "user_log = pd.read_csv(\"./data/user_log_format1.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# 对数据内存压缩\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print(\"Memory usage after optimation is {:.2f} MB\".format(end_mem))\n",
    "    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimation is 1.74 MB\n",
      "Decreased by 70.8%\n",
      "Memory usage after optimation is 3.49 MB\n",
      "Decreased by 41.7%\n",
      "Memory usage after optimation is 3.24 MB\n",
      "Decreased by 66.7%\n",
      "Memory usage after optimation is 890.48 MB\n",
      "Decreased by 69.6%\n"
     ]
    }
   ],
   "source": [
    "train_data = reduce_mem_usage(train_data)\n",
    "test_data = reduce_mem_usage(test_data)\n",
    "user_info = reduce_mem_usage(user_info)\n",
    "user_log = reduce_mem_usage(user_log)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "<module 'gc' (built-in)>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = train_data.append(test_data)\n",
    "all_data = all_data.merge(user_info, on=['user_id'], how='left')\n",
    "del train_data, test_data, user_info\n",
    "gc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "user_log = user_log.sort_values(['user_id', 'time_stamp'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  merchant_id  label  prob  age_range  gender\n",
      "0    34176         3906    0.0   NaN        6.0     0.0\n",
      "1    34176          121    0.0   NaN        6.0     0.0\n",
      "2    34176         4356    1.0   NaN        6.0     0.0\n",
      "3    34176         2217    0.0   NaN        6.0     0.0\n",
      "4   230784         4818    0.0   NaN        0.0     0.0\n",
      "   user_id  merchant_id  label  prob  age_range  gender  \\\n",
      "0    34176         3906    0.0   NaN        6.0     0.0   \n",
      "1    34176          121    0.0   NaN        6.0     0.0   \n",
      "2    34176         4356    1.0   NaN        6.0     0.0   \n",
      "3    34176         2217    0.0   NaN        6.0     0.0   \n",
      "4   230784         4818    0.0   NaN        0.0     0.0   \n",
      "\n",
      "                                           item_path  \\\n",
      "0  581818 879005 581818 581818 1011673 52343 2773...   \n",
      "1  581818 879005 581818 581818 1011673 52343 2773...   \n",
      "2  581818 879005 581818 581818 1011673 52343 2773...   \n",
      "3  581818 879005 581818 581818 1011673 52343 2773...   \n",
      "4  191923 191923 191923 191923 964906 229470 2294...   \n",
      "\n",
      "                                            cat_path  \\\n",
      "0  1505 662 1505 1505 1505 662 1095 1505 662 1095...   \n",
      "1  1505 662 1505 1505 1505 662 1095 1505 662 1095...   \n",
      "2  1505 662 1505 1505 1505 662 1095 1505 662 1095...   \n",
      "3  1505 662 1505 1505 1505 662 1095 1505 662 1095...   \n",
      "4  1023 1023 1023 1023 662 664 664 1544 664 662 6...   \n",
      "\n",
      "                                         seller_path  \\\n",
      "0  416 3606 416 416 416 3760 3606 416 1926 3004 4...   \n",
      "1  416 3606 416 416 416 3760 3606 416 1926 3004 4...   \n",
      "2  416 3606 416 416 416 3760 3606 416 1926 3004 4...   \n",
      "3  416 3606 416 416 416 3760 3606 416 1926 3004 4...   \n",
      "4  3545 3545 3545 3545 4566 2537 2537 2420 2537 4...   \n",
      "\n",
      "                                          brand_path  \\\n",
      "0  4014.0 33.0 4014.0 4014.0 4014.0 3738.0 33.0 4...   \n",
      "1  4014.0 33.0 4014.0 4014.0 4014.0 3738.0 33.0 4...   \n",
      "2  4014.0 33.0 4014.0 4014.0 4014.0 3738.0 33.0 4...   \n",
      "3  4014.0 33.0 4014.0 4014.0 4014.0 3738.0 33.0 4...   \n",
      "4  5860.0 5860.0 5860.0 5860.0 6320.0 6064.0 6064...   \n",
      "\n",
      "                                     time_stamp_path  \\\n",
      "0  521 521 521 521 521 521 521 521 521 521 521 52...   \n",
      "1  521 521 521 521 521 521 521 521 521 521 521 52...   \n",
      "2  521 521 521 521 521 521 521 521 521 521 521 52...   \n",
      "3  521 521 521 521 521 521 521 521 521 521 521 52...   \n",
      "4  601 601 601 601 614 614 614 614 614 614 618 61...   \n",
      "\n",
      "                                    action_type_path  \n",
      "0  0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 0 2 ...  \n",
      "1  0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 0 2 ...  \n",
      "2  0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 0 2 ...  \n",
      "3  0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 0 2 ...  \n",
      "4  0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0 0 0 0 0 ...  \n"
     ]
    }
   ],
   "source": [
    "list_join_func = lambda x: \" \".join([str(i) for i in x])\n",
    "\n",
    "agg_dict = {\n",
    "    'item_id': list_join_func,\n",
    "    'cat_id': list_join_func,\n",
    "    'seller_id': list_join_func,\n",
    "    'brand_id': list_join_func,\n",
    "    'time_stamp': list_join_func,\n",
    "    'action_type': list_join_func\n",
    "}\n",
    "\n",
    "rename_dict = {\n",
    "    'item_id': 'item_path',\n",
    "    'cat_id': 'cat_path',\n",
    "    'seller_id': 'seller_path',\n",
    "    'brand_id': 'brand_path',\n",
    "    'time_stamp': 'time_stamp_path',\n",
    "    'action_type': 'action_type_path'\n",
    "}\n",
    "\n",
    "def merge_list(df_ID, join_columns, df_data, agg_dict, rename_dict):\n",
    "    df_data = df_data.\\\n",
    "        groupby(join_columns).\\\n",
    "        agg(agg_dict).\\\n",
    "        reset_index().\\\n",
    "        rename(columns=rename_dict)\n",
    "    df_ID = df_ID.merge(df_data, on=join_columns, how='left')\n",
    "    return df_ID\n",
    "\n",
    "print(all_data.head())\n",
    "all_data = merge_list(all_data, 'user_id', user_log, agg_dict, rename_dict)\n",
    "print(all_data.head())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "1569"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del user_log\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def cnt_(x):\n",
    "    try:\n",
    "        return len(x.split(' '))\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def nunique_(x):\n",
    "    try:\n",
    "        return len(set(x.split(' ')))\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def max_(x):\n",
    "    try:\n",
    "        return np.max([float(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def min_(x):\n",
    "    try:\n",
    "        return np.min([float(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def std_(x):\n",
    "    try:\n",
    "        return np.std([float(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def most_n(x, n):\n",
    "    try:\n",
    "        return Counter(x.split(' ')).most_common(n)[n-1][0]\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def most_n_cnt(x, n):\n",
    "    try:\n",
    "        return Counter(x.split(' ')).most_common(n)[n-1][1]\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def user_cnt(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(cnt_)\n",
    "    return df_data\n",
    "\n",
    "def user_unique(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(nunique_)\n",
    "    return df_data\n",
    "\n",
    "def user_max(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(max_)\n",
    "    return df_data\n",
    "\n",
    "def user_min(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(min_)\n",
    "    return df_data\n",
    "\n",
    "def user_std(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(std_)\n",
    "    return df_data\n",
    "\n",
    "def user_most_n(df_data, single_col, name, n=1):\n",
    "    func = lambda x: most_n(x, n)\n",
    "    df_data[name] = df_data[single_col].apply(func)\n",
    "    return df_data\n",
    "\n",
    "def user_most_n_cnt(df_data, single_col, name, n=1):\n",
    "    func = lambda x: most_n_cnt(x, n)\n",
    "    df_data[name] = df_data[single_col].apply(func)\n",
    "    return df_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "all_data_test = all_data\n",
    "all_data_test = user_cnt(all_data_test, 'seller_path', 'user_cnt')\n",
    "all_data_test = user_unique(all_data_test, 'seller_path', 'seller_nunique')\n",
    "all_data_test = user_unique(all_data_test, 'cat_path', 'cat_nunique')\n",
    "all_data_test = user_unique(all_data_test, 'brand_path', 'brand_nunique')\n",
    "all_data_test = user_unique(all_data_test, 'item_path', 'item_nunique')\n",
    "all_data_test = user_unique(all_data_test, 'time_stamp_path', 'time_stamp_nunique')\n",
    "all_data_test = user_unique(all_data_test, 'action_type_path', 'action_type_nunique')\n",
    "\n",
    "all_data_test = user_max(all_data_test, 'action_type_path', 'time_stamp_max')\n",
    "all_data_test = user_min(all_data_test, 'action_type_path', 'time_stamp_min')\n",
    "all_data_test = user_std(all_data_test, 'action_type_path', 'time_stamp_std')\n",
    "all_data_test['time_stamp_range'] = all_data_test['time_stamp_max'] - all_data_test['time_stamp_min']\n",
    "all_data_test = user_most_n(all_data_test, 'seller_path', 'seller_most_1', 1)\n",
    "all_data_test = user_most_n(all_data_test, 'cat_path', 'cat_most_1', 1)\n",
    "all_data_test = user_most_n(all_data_test, 'brand_path', 'brand_most_1', 1)\n",
    "all_data_test = user_most_n(all_data_test, 'action_type_path', 'action_type_1', 1)\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'seller_path', 'seller_most_1_cnt', 1)\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'cat_path', 'cat_most_1_cnt', 1)\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'brand_path', 'brand_most_1_cnt', 1)\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'action_type_path', 'action_type_1_cnt', 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def col_cnt_(df_data, columns_list, action_type):\n",
    "    try:\n",
    "        data_dict = {}\n",
    "        col_list = copy.deepcopy(columns_list)\n",
    "        if action_type != None:\n",
    "            col_list += ['action_type_path']\n",
    "\n",
    "        for col in col_list:\n",
    "            data_dict[col] = df_data[col].split(' ')\n",
    "\n",
    "        path_len = len(data_dict[col])\n",
    "\n",
    "        data_out = []\n",
    "        for i_ in range(path_len):\n",
    "            data_txt = ''\n",
    "            for col_ in columns_list:\n",
    "                if data_dict['action_type_path'][i_] == action_type:\n",
    "                    data_txt += '_' + data_dict[col_][i_]\n",
    "            data_out.append(data_txt)\n",
    "        return len(data_out)\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def col_nuique_(df_data, columns_list, action_type):\n",
    "    try:\n",
    "        data_dict = {}\n",
    "\n",
    "        col_list = copy.deepcopy(columns_list)\n",
    "        if action_type != None:\n",
    "            col_list += ['action_type_path']\n",
    "\n",
    "        for col in col_list:\n",
    "            data_dict[col] = df_data[col].split(' ')\n",
    "\n",
    "        path_len = len(data_dict[col])\n",
    "\n",
    "        data_out = []\n",
    "        for i_ in range(path_len):\n",
    "            data_txt = ''\n",
    "            for col_ in columns_list:\n",
    "                if data_dict['action_type_path'][i_] == action_type:\n",
    "                    data_txt += '_' + data_dict[col_][i_]\n",
    "            data_out.append(data_txt)\n",
    "\n",
    "        return len(set(data_out))\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def user_col_cnt(df_data, columns_list, action_type, name):\n",
    "    df_data[name] = df_data.apply(lambda x: col_cnt_(x, columns_list, action_type), axis=1)\n",
    "    return df_data\n",
    "\n",
    "def user_col_nunique(df_data, columns_list, action_type, name):\n",
    "    df_data[name] = df_data.apply(lambda x: col_nuique_(x, columns_list, action_type), axis=1)\n",
    "    return df_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# 点击次数\n",
    "all_data_test = user_col_cnt(all_data_test,  ['seller_path'], '0', 'user_cnt_0')\n",
    "# 加购次数\n",
    "all_data_test = user_col_cnt(all_data_test,  ['seller_path'], '1', 'user_cnt_1')\n",
    "# 购买次数\n",
    "all_data_test = user_col_cnt(all_data_test,  ['seller_path'], '2', 'user_cnt_2')\n",
    "# 收藏次数\n",
    "all_data_test = user_col_cnt(all_data_test,  ['seller_path'], '3', 'user_cnt_3')\n",
    "\n",
    "# 不同店铺个数\n",
    "all_data_test = user_col_nunique(all_data_test,  ['seller_path'], '0', 'seller_nunique_0')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['user_id', 'merchant_id', 'label', 'prob', 'age_range', 'gender',\n       'item_path', 'cat_path', 'seller_path', 'brand_path', 'time_stamp_path',\n       'action_type_path', 'user_cnt', 'seller_nunique', 'cat_nunique',\n       'brand_nunique', 'item_nunique', 'time_stamp_nunique',\n       'action_type_nunique', 'time_stamp_max', 'time_stamp_min',\n       'time_stamp_std', 'time_stamp_range', 'seller_most_1', 'cat_most_1',\n       'brand_most_1', 'action_type_1', 'seller_most_1_cnt', 'cat_most_1_cnt',\n       'brand_most_1_cnt', 'action_type_1_cnt', 'user_cnt_0', 'user_cnt_1',\n       'user_cnt_2', 'user_cnt_3', 'seller_nunique_0'],\n      dtype='object')"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_test.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "# 利用Countvector和TF-IDF提取特征\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from scipy import sparse\n",
    "\n",
    "tfidfVec = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS,\n",
    "                           ngram_range=(1, 1),\n",
    "                           max_features=100)\n",
    "\n",
    "columns_list = ['seller_path']\n",
    "for i, col in enumerate(columns_list):\n",
    "    tfidfVec.fit(all_data_test[col])\n",
    "    data_ = tfidfVec.transform(all_data_test[col])\n",
    "    if i == 0:\n",
    "        data_cat = data_\n",
    "    else:\n",
    "        data_cat = sparse.hstack((data_cat, data_))\n",
    "\n",
    "df_tfidf = pd.DataFrame(data_cat.toarray())\n",
    "df_tfidf.columns = ['tfidf_' + str(i) for i in df_tfidf.columns]\n",
    "all_data_test = pd.concat([all_data_test, df_tfidf], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "model = gensim.models.Word2Vec(\n",
    "    all_data_test[\"seller_path\"].apply(lambda x: x.split(' ')),\n",
    "    # size = 100,\n",
    "    window = 5,\n",
    "    min_count = 5,\n",
    "    workers = 4\n",
    ")\n",
    "\n",
    "def mean_w2v_(x, model, size=100):\n",
    "    try:\n",
    "        i = 0\n",
    "        for word in x.split(' '):\n",
    "            if word in model.wv.vocab:\n",
    "                i += 1\n",
    "                if i == 1:\n",
    "                    vec = np.zeros(size)\n",
    "                vec += model.wv[word]\n",
    "        return vec / i\n",
    "    except:\n",
    "        return np.zeros(size)\n",
    "\n",
    "def get_mean_w2v(df_data, columns, model, size):\n",
    "    data_array = []\n",
    "    for index, row in df_data.iterrows():\n",
    "        w2v = mean_w2v_(row[columns], model, size)\n",
    "        data_array.append(w2v)\n",
    "    return pd.DataFrame(data_array)\n",
    "\n",
    "df_embedding = get_mean_w2v(all_data_test, 'seller_path', model, 100)\n",
    "df_embedding.columns = ['embedding_' + str(i) for i in df_embedding.columns]\n",
    "\n",
    "all_data_test = pd.concat([all_data_test, df_embedding], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "# Stacking 分类特征\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import xgboost\n",
    "import lightgbm\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier,ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor,GradientBoostingRegressor,ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import log_loss,mean_absolute_error,mean_squared_error\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}