{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import LabelBinarizer,LabelEncoder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "path  = 'data/'\n",
    "train = pd.read_csv(path + 'security_train.csv')\n",
    "test  = pd.read_csv(path + 'security_test.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "class _Data_Preprocess:\n",
    "    def __init__(self):\n",
    "        self.int8_max = np.iinfo(np.int8).max\n",
    "        self.int8_min = np.iinfo(np.int8).min\n",
    "\n",
    "        self.int16_max = np.iinfo(np.int16).max\n",
    "        self.int16_min = np.iinfo(np.int16).min\n",
    "\n",
    "        self.int32_max = np.iinfo(np.int32).max\n",
    "        self.int32_min = np.iinfo(np.int32).min\n",
    "\n",
    "        self.int64_max = np.iinfo(np.int64).max\n",
    "        self.int64_min = np.iinfo(np.int64).min\n",
    "\n",
    "        self.float16_max = np.finfo(np.float16).max\n",
    "        self.float16_min = np.finfo(np.float16).min\n",
    "\n",
    "        self.float32_max = np.finfo(np.float32).max\n",
    "        self.float32_min = np.finfo(np.float32).min\n",
    "\n",
    "        self.float64_max = np.finfo(np.float64).max\n",
    "        self.float64_min = np.finfo(np.float64).min\n",
    "\n",
    "    def _get_type(self, min_val, max_val, types):\n",
    "        if types == 'int':\n",
    "            if max_val <= self.int8_max and min_val >= self.int8_min:\n",
    "                return np.int8\n",
    "            elif max_val <= self.int16_max <= max_val and min_val >= self.int16_min:\n",
    "                return np.int16\n",
    "            elif max_val <= self.int32_max and min_val >= self.int32_min:\n",
    "                return np.int32\n",
    "            return None\n",
    "\n",
    "        elif types == 'float':\n",
    "            if max_val <= self.float16_max and min_val >= self.float16_min:\n",
    "                return np.float16\n",
    "            if max_val <= self.float32_max and min_val >= self.float32_min:\n",
    "                return np.float32\n",
    "            if max_val <= self.float64_max and min_val >= self.float64_min:\n",
    "                return np.float64\n",
    "            return None\n",
    "\n",
    "    def _memory_process(self, df):\n",
    "        init_memory = df.memory_usage().sum() / 1024 ** 2 / 1024\n",
    "        print('Original data occupies {} GB memory.'.format(init_memory))\n",
    "        df_cols = df.columns\n",
    "\n",
    "\n",
    "        for col in tqdm_notebook(df_cols):\n",
    "            try:\n",
    "                if 'float' in str(df[col].dtypes):\n",
    "                    max_val = df[col].max()\n",
    "                    min_val = df[col].min()\n",
    "                    trans_types = self._get_type(min_val, max_val, 'float')\n",
    "                    if trans_types is not None:\n",
    "                        df[col] = df[col].astype(trans_types)\n",
    "                elif 'int' in str(df[col].dtypes):\n",
    "                    max_val = df[col].max()\n",
    "                    min_val = df[col].min()\n",
    "                    trans_types = self._get_type(min_val, max_val, 'int')\n",
    "                    if trans_types is not None:\n",
    "                        df[col] = df[col].astype(trans_types)\n",
    "            except:\n",
    "                print(' Can not do any process for column, {}.'.format(col))\n",
    "        afterprocess_memory = df.memory_usage().sum() / 1024 ** 2 / 1024\n",
    "        print('After processing, the data occupies {} GB memory.'.format(afterprocess_memory))\n",
    "        return df\n",
    "\n",
    "memory_process = _Data_Preprocess()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# （字符串转化为数字）\n",
    "unique_api = train['api'].unique()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "api2index = {item:(i+1) for i,item in enumerate(unique_api)}\n",
    "index2api = {(i+1):item for i,item in enumerate(unique_api)}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "train['api_idx'] = train['api'].map(api2index)\n",
    "test['api_idx']  = test['api'].map(api2index)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# 获取每个文件对应的字符串序列\n",
    "def get_sequence(df,period_idx):\n",
    "    seq_list = []\n",
    "    for _id,begin in enumerate(period_idx[:-1]):\n",
    "        seq_list.append(df.iloc[begin:period_idx[_id+1]]['api_idx'].values)\n",
    "    seq_list.append(df.iloc[period_idx[-1]:]['api_idx'].values)\n",
    "    return seq_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "train_period_idx = train.file_id.drop_duplicates(keep='first').index.values\n",
    "test_period_idx  = test.file_id.drop_duplicates(keep='first').index.values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "train_df = train[['file_id','label']].drop_duplicates(keep='first')\n",
    "test_df  = test[['file_id']].drop_duplicates(keep='first')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "train_df['seq'] = get_sequence(train,train_period_idx)\n",
    "test_df['seq']  = get_sequence(test,test_period_idx)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Lambda, Embedding, Dropout, Activation,GRU,Bidirectional\n",
    "from keras.layers import Conv1D,Conv2D,MaxPooling2D,GlobalAveragePooling1D,GlobalMaxPooling1D, MaxPooling1D, Flatten\n",
    "from keras.layers import GRU, LSTM, SpatialDropout1D\n",
    "from keras.layers.merge import concatenate, Concatenate, Average, Dot, Maximum, Multiply, Subtract, average\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, LatentDirichletAllocation\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.layers.wrappers import Bidirectional"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def TextCNN(max_len,max_cnt,embed_size, num_filters,kernel_size,conv_action, mask_zero):\n",
    "\n",
    "    _input = Input(shape=(max_len,), dtype='int32')\n",
    "    _embed = Embedding(max_cnt, embed_size, input_length=max_len, mask_zero=mask_zero)(_input)\n",
    "    _embed = SpatialDropout1D(0.15)(_embed)\n",
    "    warppers = []\n",
    "\n",
    "    for _kernel_size in kernel_size:\n",
    "        conv1d = Conv1D(filters=num_filters, kernel_size=_kernel_size, activation=conv_action)(_embed)\n",
    "        warppers.append(GlobalMaxPooling1D()(conv1d))\n",
    "\n",
    "    fc = concatenate(warppers)\n",
    "    fc = Dropout(0.5)(fc)\n",
    "    #fc = BatchNormalization()(fc)\n",
    "    fc = Dense(256, activation='relu')(fc)\n",
    "    fc = Dropout(0.25)(fc)\n",
    "    #fc = BatchNormalization()(fc)\n",
    "    preds = Dense(8, activation = 'softmax')(fc)\n",
    "\n",
    "    model = Model(inputs=_input, outputs=preds)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy'])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "train_labels = pd.get_dummies(train_df.label).values\n",
    "train_seq    = pad_sequences(train_df.seq.values, maxlen = 6000)\n",
    "test_seq     = pad_sequences(test_df.seq.values, maxlen = 6000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "skf = KFold(n_splits=5, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "max_len     = 6000\n",
    "max_cnt     = 295\n",
    "embed_size  = 256\n",
    "num_filters = 64\n",
    "kernel_size = [2,4,6,8,10,12,14]\n",
    "conv_action = 'relu'\n",
    "mask_zero   = False\n",
    "TRAIN       = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: \n",
      "2778 11109\n",
      "Epoch 1/100\n",
      " 10/174 [>.............................] - ETA: 37:11 - loss: 1.7302 - accuracy: 0.3797\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    }
   ],
   "source": [
    "import os\n",
    "meta_train = np.zeros(shape = (len(train_seq),8))\n",
    "meta_test = np.zeros(shape = (len(test_seq),8))\n",
    "FLAG = True\n",
    "i = 0\n",
    "for tr_ind,te_ind in skf.split(train_labels):\n",
    "    i +=1\n",
    "    print('FOLD: '.format(i))\n",
    "    print(len(te_ind),len(tr_ind))\n",
    "    model_name = 'benchmark_textcnn_fold_'+str(i)\n",
    "    X_train,X_train_label = train_seq[tr_ind],train_labels[tr_ind]\n",
    "    X_val,X_val_label     = train_seq[te_ind],train_labels[te_ind]\n",
    "\n",
    "    model = TextCNN(max_len,max_cnt,embed_size,num_filters,kernel_size,conv_action,mask_zero)\n",
    "\n",
    "    model_save_path = './NN/%s_%s.hdf5'%(model_name,embed_size)\n",
    "    early_stopping =EarlyStopping(monitor='val_loss', patience=3)\n",
    "    model_checkpoint = ModelCheckpoint(model_save_path, save_best_only=True, save_weights_only=True)\n",
    "    if TRAIN and FLAG:\n",
    "        model.fit(X_train,X_train_label,validation_data=(X_val,X_val_label),epochs=100,batch_size=64,shuffle=True,callbacks=[early_stopping,model_checkpoint] )\n",
    "\n",
    "    model.load_weights(model_save_path)\n",
    "    pred_val = model.predict(X_val,batch_size=128,verbose=1)\n",
    "    pred_test = model.predict(test_seq,batch_size=128,verbose=1)\n",
    "\n",
    "    meta_train[te_ind] = pred_val\n",
    "    meta_test += pred_test\n",
    "    K.clear_session()\n",
    "meta_test /= 5.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_df['prob0'] = 0\n",
    "test_df['prob1'] = 0\n",
    "test_df['prob2'] = 0\n",
    "test_df['prob3'] = 0\n",
    "test_df['prob4'] = 0\n",
    "test_df['prob5'] = 0\n",
    "test_df['prob6'] = 0\n",
    "test_df['prob7'] = 0\n",
    "\n",
    "test_df[['prob0','prob1','prob2','prob3','prob4','prob5','prob6','prob7']] = meta_test\n",
    "test_df[['file_id','prob0','prob1','prob2','prob3','prob4','prob5','prob6','prob7']].to_csv('nn_baseline_5fold.csv',index = None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}